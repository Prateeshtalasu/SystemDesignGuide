# âš ï¸ Caching Anti-Patterns

---

## 0ï¸âƒ£ Prerequisites

Before studying caching anti-patterns, you should understand:

- **Caching Patterns**: Cache-Aside, Write-Through, Write-Behind. Covered in Topic 1.
- **Cache Invalidation**: TTL, event-based, version-based. Covered in Topic 2.
- **Cache Stampede**: Thundering herd problem. Covered in Topic 5.
- **HTTP Caching**: Cache-Control headers. Covered in Topic 10.

If you've implemented caching before and want to avoid common pitfalls, you're ready.

---

## 1ï¸âƒ£ What Problem Does This Exist to Solve?

### The Pain Point

Caching seems simple: store data, retrieve it faster. But incorrect caching can cause:

```mermaid
flowchart TD
    Title["CACHING GONE WRONG"]
    
    Problems["âŒ Stale data shown to users (lost sales, wrong prices)<br/>âŒ Cache stampedes crashing your database<br/>âŒ Memory exhaustion from unbounded caches<br/>âŒ Security breaches from cached private data<br/>âŒ Debugging nightmares from inconsistent state<br/>âŒ Performance worse than no caching at all"]
    
    Quote["There are only two hard things in Computer Science:<br/>cache invalidation and naming things.<br/>- Phil Karlton"]
    
    Note["This topic covers the common mistakes and how to avoid them."]
```

<details>
<summary>ASCII diagram (reference)</summary>

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CACHING GONE WRONG                                    â”‚
â”‚                                                                          â”‚
â”‚   âŒ Stale data shown to users (lost sales, wrong prices)               â”‚
â”‚   âŒ Cache stampedes crashing your database                             â”‚
â”‚   âŒ Memory exhaustion from unbounded caches                            â”‚
â”‚   âŒ Security breaches from cached private data                         â”‚
â”‚   âŒ Debugging nightmares from inconsistent state                       â”‚
â”‚   âŒ Performance worse than no caching at all                           â”‚
â”‚                                                                          â”‚
â”‚   "There are only two hard things in Computer Science:                  â”‚
â”‚    cache invalidation and naming things."                               â”‚
â”‚                                    - Phil Karlton                        â”‚
â”‚                                                                          â”‚
â”‚   This topic covers the common mistakes and how to avoid them.          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
</details>

---

## 2ï¸âƒ£ Anti-Pattern #1: Caching Mutable Data Without Invalidation

### The Problem

```mermaid
flowchart TD
    Title["STALE DATA DISASTER - Timeline"]
    
    T0["T=0: Product price = $100<br/>Cache: product:123 â†’ {price: $100}<br/>TTL: 1 hour"]
    
    T5["T=5min: Admin updates price to $80 (20% off sale!)<br/>Database: price = $80<br/>Cache: Still {price: $100} â† NOT INVALIDATED"]
    
    T6["T=6min: Customer sees $100, goes to competitor"]
    T10["T=10min: Another customer sees $100, complains"]
    T55["T=55min: Cache finally expires, shows $80"]
    
    Result["Result: 55 minutes of lost sales and angry customers"]
    
    T0 --> T5 --> T6 --> T10 --> T55 --> Result
```

<details>
<summary>ASCII diagram (reference)</summary>

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STALE DATA DISASTER                                   â”‚
â”‚                                                                          â”‚
â”‚   Timeline:                                                              â”‚
â”‚                                                                          â”‚
â”‚   T=0:  Product price = $100                                            â”‚
â”‚         Cache: product:123 â†’ {price: $100}                              â”‚
â”‚         TTL: 1 hour                                                      â”‚
â”‚                                                                          â”‚
â”‚   T=5min: Admin updates price to $80 (20% off sale!)                   â”‚
â”‚           Database: price = $80                                          â”‚
â”‚           Cache: Still {price: $100} â† NOT INVALIDATED                  â”‚
â”‚                                                                          â”‚
â”‚   T=6min: Customer sees $100, goes to competitor                        â”‚
â”‚   T=10min: Another customer sees $100, complains                        â”‚
â”‚   T=55min: Cache finally expires, shows $80                             â”‚
â”‚                                                                          â”‚
â”‚   Result: 55 minutes of lost sales and angry customers                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
</details>

### Bad Code

```java
// âŒ ANTI-PATTERN: Cache without invalidation
@Service
public class ProductService {
    
    @Cacheable(value = "products", key = "#id")
    public Product getProduct(Long id) {
        return productRepository.findById(id).orElse(null);
    }
    
    // NO @CacheEvict! Updates don't invalidate cache!
    public Product updateProduct(Long id, ProductUpdateRequest request) {
        Product product = productRepository.findById(id).orElseThrow();
        product.setPrice(request.getPrice());
        return productRepository.save(product);
        // Cache still has old price!
    }
}
```

### Good Code

```java
// âœ… CORRECT: Always invalidate on mutation
@Service
public class ProductService {
    
    @Cacheable(value = "products", key = "#id")
    public Product getProduct(Long id) {
        return productRepository.findById(id).orElse(null);
    }
    
    @CacheEvict(value = "products", key = "#id")
    public Product updateProduct(Long id, ProductUpdateRequest request) {
        Product product = productRepository.findById(id).orElseThrow();
        product.setPrice(request.getPrice());
        return productRepository.save(product);
    }
    
    @CacheEvict(value = "products", key = "#id")
    public void deleteProduct(Long id) {
        productRepository.deleteById(id);
    }
    
    // For batch updates, evict all
    @CacheEvict(value = "products", allEntries = true)
    public void bulkUpdatePrices(List<PriceUpdate> updates) {
        // ... bulk update logic
    }
}
```

### Even Better: Event-Driven Invalidation

```java
// âœ… BEST: Decouple invalidation from business logic
@Service
public class ProductService {
    
    private final ApplicationEventPublisher eventPublisher;
    
    public Product updateProduct(Long id, ProductUpdateRequest request) {
        Product product = productRepository.findById(id).orElseThrow();
        product.setPrice(request.getPrice());
        Product saved = productRepository.save(product);
        
        // Publish event - cache invalidation happens elsewhere
        eventPublisher.publishEvent(new ProductUpdatedEvent(id));
        
        return saved;
    }
}

@Component
public class CacheInvalidationListener {
    
    private final CacheManager cacheManager;
    
    @EventListener
    public void onProductUpdated(ProductUpdatedEvent event) {
        Cache cache = cacheManager.getCache("products");
        if (cache != null) {
            cache.evict(event.getProductId());
        }
    }
}
```

---

## 3ï¸âƒ£ Anti-Pattern #2: Over-Caching (Everything in Cache)

### The Problem

```mermaid
flowchart TD
    Title["OVER-CACHING<br/>Let's cache EVERYTHING for performance!"]
    
    subgraph Problems["Problems"]
        P1["1. Memory Exhaustion<br/>- 10 million users Ã— 1KB profile = 10GB cache<br/>- Redis runs out of memory<br/>- Eviction starts, cache becomes useless"]
        
        P2["2. Stale Data Everywhere<br/>- More cached data = more to invalidate<br/>- Miss one invalidation = stale data<br/>- Debugging becomes impossible"]
        
        P3["3. Cold Start Nightmare<br/>- Server restart = empty cache<br/>- Database hammered while cache warms up<br/>- Takes hours to reach steady state"]
        
        P4["4. Diminishing Returns<br/>- Caching data accessed once/day: wasted memory<br/>- 80/20 rule: 20% of data serves 80% of requests"]
    end
```

<details>
<summary>ASCII diagram (reference)</summary>

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    OVER-CACHING                                          â”‚
â”‚                                                                          â”‚
â”‚   "Let's cache EVERYTHING for performance!"                             â”‚
â”‚                                                                          â”‚
â”‚   Problems:                                                              â”‚
â”‚                                                                          â”‚
â”‚   1. Memory Exhaustion                                                   â”‚
â”‚      - 10 million users Ã— 1KB profile = 10GB cache                      â”‚
â”‚      - Redis runs out of memory                                          â”‚
â”‚      - Eviction starts, cache becomes useless                           â”‚
â”‚                                                                          â”‚
â”‚   2. Stale Data Everywhere                                              â”‚
â”‚      - More cached data = more to invalidate                            â”‚
â”‚      - Miss one invalidation = stale data                               â”‚
â”‚      - Debugging becomes impossible                                      â”‚
â”‚                                                                          â”‚
â”‚   3. Cold Start Nightmare                                               â”‚
â”‚      - Server restart = empty cache                                      â”‚
â”‚      - Database hammered while cache warms up                           â”‚
â”‚      - Takes hours to reach steady state                                â”‚
â”‚                                                                          â”‚
â”‚   4. Diminishing Returns                                                â”‚
â”‚      - Caching data accessed once/day: wasted memory                    â”‚
â”‚      - 80/20 rule: 20% of data serves 80% of requests                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
</details>

### Bad Code

```java
// âŒ ANTI-PATTERN: Cache everything
@Service
public class UserService {
    
    // Caches ALL user profiles - millions of entries!
    @Cacheable("users")
    public User getUser(Long id) {
        return userRepository.findById(id).orElse(null);
    }
    
    // Caches ALL user preferences
    @Cacheable("preferences")
    public UserPreferences getPreferences(Long userId) {
        return preferencesRepository.findByUserId(userId);
    }
    
    // Caches ALL user activity logs (huge!)
    @Cacheable("activity")
    public List<ActivityLog> getActivityLog(Long userId) {
        return activityRepository.findByUserId(userId);
    }
    
    // Caches rarely-accessed audit data
    @Cacheable("audit")
    public List<AuditEntry> getAuditHistory(Long userId) {
        return auditRepository.findByUserId(userId);
    }
}
```

### Good Code

```java
// âœ… CORRECT: Cache strategically based on access patterns
@Service
public class UserService {
    
    // Cache hot data with bounded size
    @Cacheable(value = "users", unless = "#result == null")
    public User getUser(Long id) {
        return userRepository.findById(id).orElse(null);
    }
    
    // Don't cache rarely accessed data
    public List<AuditEntry> getAuditHistory(Long userId) {
        // Accessed rarely, large data - don't cache
        return auditRepository.findByUserId(userId);
    }
}

// Configuration with bounded cache
@Configuration
public class CacheConfig {
    
    @Bean
    public CacheManager cacheManager() {
        CaffeineCacheManager manager = new CaffeineCacheManager();
        manager.setCaffeine(Caffeine.newBuilder()
            .maximumSize(10_000)        // Bound the cache size!
            .expireAfterWrite(Duration.ofMinutes(10))
            .recordStats());            // Monitor hit rates
        return manager;
    }
}
```

### Decision Framework

```mermaid
flowchart TD
    Title["SHOULD I CACHE THIS?<br/>Ask these questions:"]
    
    Q1["1. How often is it accessed?<br/>- Multiple times per second â†’ YES, cache<br/>- Once per day â†’ Probably NO"]
    
    Q2["2. How expensive is it to compute/fetch?<br/>- Complex query, external API â†’ YES, cache<br/>- Simple primary key lookup â†’ Maybe not worth it"]
    
    Q3["3. How often does it change?<br/>- Rarely changes â†’ YES, cache with long TTL<br/>- Changes every second â†’ Probably NO"]
    
    Q4["4. Is stale data acceptable?<br/>- Yes (news, recommendations) â†’ YES, cache<br/>- No (inventory, prices) â†’ Cache carefully with invalidation"]
    
    Q5["5. How big is the data?<br/>- Small (< 1KB) â†’ YES, cache<br/>- Large (> 100KB) â†’ Consider if worth the memory"]
    
    Rule["Rule of thumb: Start without cache, add when you have evidence<br/>of performance problems."]
    
    Title --> Q1 --> Q2 --> Q3 --> Q4 --> Q5 --> Rule
```

<details>
<summary>ASCII diagram (reference)</summary>

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SHOULD I CACHE THIS?                                  â”‚
â”‚                                                                          â”‚
â”‚   Ask these questions:                                                   â”‚
â”‚                                                                          â”‚
â”‚   1. How often is it accessed?                                          â”‚
â”‚      - Multiple times per second â†’ YES, cache                           â”‚
â”‚      - Once per day â†’ Probably NO                                       â”‚
â”‚                                                                          â”‚
â”‚   2. How expensive is it to compute/fetch?                              â”‚
â”‚      - Complex query, external API â†’ YES, cache                         â”‚
â”‚      - Simple primary key lookup â†’ Maybe not worth it                   â”‚
â”‚                                                                          â”‚
â”‚   3. How often does it change?                                          â”‚
â”‚      - Rarely changes â†’ YES, cache with long TTL                        â”‚
â”‚      - Changes every second â†’ Probably NO                               â”‚
â”‚                                                                          â”‚
â”‚   4. Is stale data acceptable?                                          â”‚
â”‚      - Yes (news, recommendations) â†’ YES, cache                         â”‚
â”‚      - No (inventory, prices) â†’ Cache carefully with invalidation       â”‚
â”‚                                                                          â”‚
â”‚   5. How big is the data?                                               â”‚
â”‚      - Small (< 1KB) â†’ YES, cache                                       â”‚
â”‚      - Large (> 100KB) â†’ Consider if worth the memory                   â”‚
â”‚                                                                          â”‚
â”‚   Rule of thumb: Start without cache, add when you have evidence        â”‚
â”‚   of performance problems.                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
</details>

---

## 4ï¸âƒ£ Anti-Pattern #3: Under-Caching (No Cache Strategy)

### The Problem

```mermaid
flowchart TD
    Title["UNDER-CACHING<br/>We'll add caching later when we need it"]
    
    Reality["Reality:<br/>- Database gets hammered with same queries<br/>- External APIs rate-limit you<br/>- Response times increase under load<br/>- You scramble to add caching during an outage<br/>- Hastily added cache causes more problems"]
    
    Example["Example:<br/>Homepage loads user profile, preferences, notifications,<br/>recommendations - 5 DB queries per page load.<br/>1000 users = 5000 queries/second to database<br/>Database maxes out at 10,000 queries/second<br/>At 2000 users, site goes down"]
    
    WithCache["With caching:<br/>95% cache hit rate = 250 queries/second<br/>Can handle 40,000 users before database limit"]
    
    Title --> Reality --> Example --> WithCache
```

<details>
<summary>ASCII diagram (reference)</summary>

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    UNDER-CACHING                                         â”‚
â”‚                                                                          â”‚
â”‚   "We'll add caching later when we need it"                             â”‚
â”‚                                                                          â”‚
â”‚   Reality:                                                               â”‚
â”‚                                                                          â”‚
â”‚   - Database gets hammered with same queries                            â”‚
â”‚   - External APIs rate-limit you                                        â”‚
â”‚   - Response times increase under load                                  â”‚
â”‚   - You scramble to add caching during an outage                        â”‚
â”‚   - Hastily added cache causes more problems                            â”‚
â”‚                                                                          â”‚
â”‚   Example:                                                               â”‚
â”‚   Homepage loads user profile, preferences, notifications,              â”‚
â”‚   recommendations - 5 DB queries per page load.                         â”‚
â”‚   1000 users = 5000 queries/second to database                          â”‚
â”‚   Database maxes out at 10,000 queries/second                           â”‚
â”‚   At 2000 users, site goes down                                         â”‚
â”‚                                                                          â”‚
â”‚   With caching:                                                          â”‚
â”‚   95% cache hit rate = 250 queries/second                               â”‚
â”‚   Can handle 40,000 users before database limit                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
</details>

### Bad Code

```java
// âŒ ANTI-PATTERN: No caching at all
@RestController
public class HomeController {
    
    @GetMapping("/home")
    public HomePageData getHomePage(@AuthUser User user) {
        // Every single request hits the database
        User profile = userService.getUser(user.getId());           // DB hit
        List<Notification> notifs = notificationService.get(user);  // DB hit
        List<Product> recommended = recommendationService.get(user); // DB hit + ML
        List<Order> orders = orderService.getRecent(user);          // DB hit
        
        return new HomePageData(profile, notifs, recommended, orders);
    }
}
```

### Good Code

```java
// âœ… CORRECT: Strategic caching
@RestController
public class HomeController {
    
    @GetMapping("/home")
    public HomePageData getHomePage(@AuthUser User user) {
        Long userId = user.getId();
        
        // User profile - cached, rarely changes
        User profile = userService.getUser(userId);
        
        // Notifications - cached briefly (1 min)
        List<Notification> notifs = notificationService.getCached(userId);
        
        // Recommendations - cached longer (computed daily)
        List<Product> recommended = recommendationService.getCached(userId);
        
        // Recent orders - cached briefly
        List<Order> orders = orderService.getRecentCached(userId);
        
        return new HomePageData(profile, notifs, recommended, orders);
    }
}

@Service
public class RecommendationService {
    
    // Expensive computation, cache for 1 hour
    @Cacheable(value = "recommendations", key = "#userId")
    public List<Product> getCached(Long userId) {
        // This expensive ML computation only runs on cache miss
        return computeRecommendations(userId);
    }
}
```

---

## 5ï¸âƒ£ Anti-Pattern #4: Cache Key Collisions

### The Problem

```mermaid
sequenceDiagram
    participant UserA
    participant Server
    participant Cache
    participant UserB
    
    Note over UserA,Cache: User A requests: /api/user/profile
    UserA->>Server: GET /api/user/profile
    Server->>Cache: Cache key: profile
    Cache-->>Server: MISS
    Server->>Server: Fetch User A's profile
    Server->>Cache: Store: User A's profile
    Server-->>UserA: User A's profile
    
    Note over UserB,Cache: User B requests: /api/user/profile
    UserB->>Server: GET /api/user/profile
    Server->>Cache: Cache key: profile (same!)
    Cache-->>Server: HIT - Returns User A's profile! ğŸ˜±
    Server-->>UserB: User A's profile
    
    Note over UserB,Cache: User B now sees User A's private data!<br/>This is a SECURITY VULNERABILITY.
```

<details>
<summary>ASCII diagram (reference)</summary>

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CACHE KEY COLLISION                                   â”‚
â”‚                                                                          â”‚
â”‚   User A requests: /api/user/profile                                    â”‚
â”‚   Cache key: "profile"                                                   â”‚
â”‚   Stored: User A's profile                                              â”‚
â”‚                                                                          â”‚
â”‚   User B requests: /api/user/profile                                    â”‚
â”‚   Cache key: "profile" (same!)                                          â”‚
â”‚   Returns: User A's profile! ğŸ˜±                                         â”‚
â”‚                                                                          â”‚
â”‚   User B now sees User A's private data!                                â”‚
â”‚   This is a SECURITY VULNERABILITY.                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
</details>

### Bad Code

```java
// âŒ ANTI-PATTERN: Non-unique cache keys
@Service
public class UserService {
    
    // Key doesn't include user ID!
    @Cacheable(value = "profile")  // Same key for ALL users!
    public UserProfile getProfile(Long userId) {
        return profileRepository.findByUserId(userId);
    }
    
    // Key collision between different entities
    @Cacheable(value = "data", key = "#id")
    public Product getProduct(Long id) { ... }
    
    @Cacheable(value = "data", key = "#id")  // Same cache, same key pattern!
    public User getUser(Long id) { ... }
    // Product 123 and User 123 collide!
}
```

### Good Code

```java
// âœ… CORRECT: Unique, namespaced cache keys
@Service
public class UserService {
    
    // Include user ID in key
    @Cacheable(value = "profiles", key = "#userId")
    public UserProfile getProfile(Long userId) {
        return profileRepository.findByUserId(userId);
    }
}

@Service
public class ProductService {
    
    // Separate cache namespace
    @Cacheable(value = "products", key = "#id")
    public Product getProduct(Long id) { ... }
}

// Or use explicit key generation
@Service
public class DataService {
    
    @Cacheable(value = "data", key = "'product:' + #id")
    public Product getProduct(Long id) { ... }
    
    @Cacheable(value = "data", key = "'user:' + #id")
    public User getUser(Long id) { ... }
}
```

### Custom Key Generator

```java
// âœ… BEST: Custom key generator for complex scenarios
@Component
public class SecureCacheKeyGenerator implements KeyGenerator {
    
    @Override
    public Object generate(Object target, Method method, Object... params) {
        StringBuilder key = new StringBuilder();
        
        // Include class and method name
        key.append(target.getClass().getSimpleName());
        key.append(".");
        key.append(method.getName());
        
        // Include all parameters
        for (Object param : params) {
            key.append(":");
            key.append(param != null ? param.toString() : "null");
        }
        
        return key.toString();
    }
}

// Usage
@Cacheable(value = "data", keyGenerator = "secureCacheKeyGenerator")
public UserProfile getProfile(Long userId, String region) {
    // Key: "UserService.getProfile:123:us-east"
}
```

---

## 6ï¸âƒ£ Anti-Pattern #5: Ignoring Cache Stampede

### The Problem

```mermaid
flowchart TD
    Title["CACHE STAMPEDE<br/>Popular item cached with 1-hour TTL<br/>1000 requests/second for this item"]
    
    T1["T=59:59 - Cache valid, all requests served from cache"]
    T2["T=60:00 - Cache expires"]
    T3["T=60:01 - 1000 requests all see cache miss<br/>1000 requests ALL hit database simultaneously!"]
    
    subgraph Stampede[""]
        R1["Request 1"]
        R2["Request 2"]
        R3["Request 3"]
        R1000["Request 1000"]
        
        CacheMiss["Cache Miss"]
        Database["Database"]
        Crash["CRASH!"]
        
        R1 & R2 & R3 & R1000 --> CacheMiss --> Database --> Crash
    end
    
    Note["All 1000 requests execute the same expensive query!<br/>This is covered in detail in Topic 5, but here's the summary:"]
    
    Title --> T1 --> T2 --> T3 --> Stampede --> Note
```

<details>
<summary>ASCII diagram (reference)</summary>

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CACHE STAMPEDE                                        â”‚
â”‚                                                                          â”‚
â”‚   Popular item cached with 1-hour TTL                                   â”‚
â”‚   1000 requests/second for this item                                    â”‚
â”‚                                                                          â”‚
â”‚   T=59:59 - Cache valid, all requests served from cache                â”‚
â”‚   T=60:00 - Cache expires                                               â”‚
â”‚   T=60:01 - 1000 requests all see cache miss                           â”‚
â”‚             1000 requests ALL hit database simultaneously!              â”‚
â”‚                                                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                                                                  â”‚   â”‚
â”‚   â”‚   Request 1 â”€â”€â”                                                  â”‚   â”‚
â”‚   â”‚   Request 2 â”€â”€â”¼â”€â”€â–¶ Cache Miss â”€â”€â”¬â”€â”€â–¶ Database â”€â”€â–¶ CRASH!       â”‚   â”‚
â”‚   â”‚   Request 3 â”€â”€â”¤                 â”‚                                â”‚   â”‚
â”‚   â”‚   ...         â”‚                 â”‚                                â”‚   â”‚
â”‚   â”‚   Request 1000â”˜                 â”‚                                â”‚   â”‚
â”‚   â”‚                                                                  â”‚   â”‚
â”‚   â”‚   All 1000 requests execute the same expensive query!           â”‚   â”‚
â”‚   â”‚                                                                  â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                          â”‚
â”‚   This is covered in detail in Topic 5, but here's the summary:        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
</details>

### Bad Code

```java
// âŒ ANTI-PATTERN: No stampede protection
@Service
public class ProductService {
    
    @Cacheable(value = "products", key = "#id")
    public Product getProduct(Long id) {
        // 1000 concurrent cache misses = 1000 DB queries
        return productRepository.findById(id).orElse(null);
    }
}
```

### Good Code

```java
// âœ… CORRECT: Stampede protection with locking
@Service
public class ProductService {
    
    private final RedisTemplate<String, Object> redis;
    private final ProductRepository repository;
    
    public Product getProduct(Long id) {
        String cacheKey = "product:" + id;
        String lockKey = "lock:" + cacheKey;
        
        // Try cache first
        Product cached = (Product) redis.opsForValue().get(cacheKey);
        if (cached != null) {
            return cached;
        }
        
        // Try to acquire lock
        Boolean acquired = redis.opsForValue()
            .setIfAbsent(lockKey, "1", Duration.ofSeconds(10));
        
        if (Boolean.TRUE.equals(acquired)) {
            try {
                // Double-check cache (another thread might have populated it)
                cached = (Product) redis.opsForValue().get(cacheKey);
                if (cached != null) {
                    return cached;
                }
                
                // Fetch from DB and cache
                Product product = repository.findById(id).orElse(null);
                if (product != null) {
                    redis.opsForValue().set(cacheKey, product, Duration.ofHours(1));
                }
                return product;
            } finally {
                redis.delete(lockKey);
            }
        } else {
            // Wait for other thread to populate cache
            for (int i = 0; i < 50; i++) {
                try { Thread.sleep(100); } catch (InterruptedException e) { break; }
                cached = (Product) redis.opsForValue().get(cacheKey);
                if (cached != null) {
                    return cached;
                }
            }
            // Timeout - fetch from DB as fallback
            return repository.findById(id).orElse(null);
        }
    }
}
```

---

## 7ï¸âƒ£ Anti-Pattern #6: Caching Without Monitoring

### The Problem

```mermaid
flowchart TD
    Title["BLIND CACHING<br/>We added caching, it must be working!"]
    
    Reality["Reality without monitoring:<br/>- Cache hit rate might be 5% (useless)<br/>- Memory might be 99% full (about to evict everything)<br/>- Latency might be worse (serialization overhead)<br/>- You have no idea if caching is helping or hurting"]
    
    Metrics["Key metrics to monitor:<br/>- Hit rate (should be > 80% for most caches)<br/>- Memory usage (should have headroom)<br/>- Eviction rate (high = cache too small)<br/>- Latency (cache should be faster than source)<br/>- Error rate (connection failures, timeouts)"]
    
    Title --> Reality --> Metrics
```

<details>
<summary>ASCII diagram (reference)</summary>

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BLIND CACHING                                         â”‚
â”‚                                                                          â”‚
â”‚   "We added caching, it must be working!"                               â”‚
â”‚                                                                          â”‚
â”‚   Reality without monitoring:                                            â”‚
â”‚                                                                          â”‚
â”‚   - Cache hit rate might be 5% (useless)                                â”‚
â”‚   - Memory might be 99% full (about to evict everything)                â”‚
â”‚   - Latency might be worse (serialization overhead)                     â”‚
â”‚   - You have no idea if caching is helping or hurting                   â”‚
â”‚                                                                          â”‚
â”‚   Key metrics to monitor:                                                â”‚
â”‚   - Hit rate (should be > 80% for most caches)                          â”‚
â”‚   - Memory usage (should have headroom)                                 â”‚
â”‚   - Eviction rate (high = cache too small)                              â”‚
â”‚   - Latency (cache should be faster than source)                        â”‚
â”‚   - Error rate (connection failures, timeouts)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
</details>

### Good Code

```java
// âœ… CORRECT: Cache with monitoring
@Configuration
public class CacheConfig {
    
    @Bean
    public CacheManager cacheManager(MeterRegistry meterRegistry) {
        CaffeineCacheManager manager = new CaffeineCacheManager();
        manager.setCaffeine(Caffeine.newBuilder()
            .maximumSize(10_000)
            .expireAfterWrite(Duration.ofMinutes(10))
            .recordStats());  // Enable stats!
        
        // Expose metrics
        manager.getCacheNames().forEach(name -> {
            Cache cache = manager.getCache(name);
            if (cache != null && cache.getNativeCache() instanceof com.github.benmanes.caffeine.cache.Cache) {
                var caffeineCache = (com.github.benmanes.caffeine.cache.Cache<?, ?>) cache.getNativeCache();
                CaffeineCacheMetrics.monitor(meterRegistry, caffeineCache, name);
            }
        });
        
        return manager;
    }
}

// Custom metrics for Redis
@Component
public class RedisCacheMetrics {
    
    private final RedisTemplate<String, Object> redis;
    private final MeterRegistry meterRegistry;
    
    @Scheduled(fixedRate = 60000)
    public void recordMetrics() {
        RedisConnection connection = redis.getConnectionFactory().getConnection();
        Properties info = connection.info("stats");
        
        // Record hit rate
        long hits = Long.parseLong(info.getProperty("keyspace_hits", "0"));
        long misses = Long.parseLong(info.getProperty("keyspace_misses", "0"));
        double hitRate = hits + misses > 0 ? (double) hits / (hits + misses) : 0;
        
        meterRegistry.gauge("redis.cache.hit_rate", hitRate);
        meterRegistry.gauge("redis.cache.hits", hits);
        meterRegistry.gauge("redis.cache.misses", misses);
        
        // Record memory
        Properties memory = connection.info("memory");
        long usedMemory = Long.parseLong(memory.getProperty("used_memory", "0"));
        meterRegistry.gauge("redis.memory.used", usedMemory);
    }
}
```

---

## 8ï¸âƒ£ Anti-Pattern #7: Caching Errors

### The Problem

```mermaid
flowchart TD
    Title["CACHING ERRORS"]
    
    Scenario1["Database temporarily down<br/>Query returns error/null<br/>Error gets cached!"]
    
    Problem1["Database comes back up<br/>Cache still serving error for 1 hour<br/>Users see errors even though system is healthy"]
    
    Scenario2["Or worse: caching null as valid data<br/>User doesn't exist â†’ cache null<br/>User created â†’ cache still returns null<br/>User can't log in!"]
    
    Title --> Scenario1 --> Problem1
    Title --> Scenario2
```

<details>
<summary>ASCII diagram (reference)</summary>

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CACHING ERRORS                                        â”‚
â”‚                                                                          â”‚
â”‚   Database temporarily down                                              â”‚
â”‚   Query returns error/null                                               â”‚
â”‚   Error gets cached!                                                     â”‚
â”‚                                                                          â”‚
â”‚   Database comes back up                                                 â”‚
â”‚   Cache still serving error for 1 hour                                  â”‚
â”‚   Users see errors even though system is healthy                        â”‚
â”‚                                                                          â”‚
â”‚   Or worse: caching null as valid data                                  â”‚
â”‚   User doesn't exist â†’ cache null                                       â”‚
â”‚   User created â†’ cache still returns null                               â”‚
â”‚   User can't log in!                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
</details>

### Bad Code

```java
// âŒ ANTI-PATTERN: Caching errors and nulls
@Service
public class UserService {
    
    @Cacheable("users")  // Caches null results!
    public User getUser(Long id) {
        try {
            return userRepository.findById(id).orElse(null);
        } catch (Exception e) {
            return null;  // Error cached as null!
        }
    }
}
```

### Good Code

```java
// âœ… CORRECT: Don't cache errors or nulls
@Service
public class UserService {
    
    // Don't cache null results
    @Cacheable(value = "users", unless = "#result == null")
    public User getUser(Long id) {
        return userRepository.findById(id).orElse(null);
    }
    
    // Or throw exception (not cached by default)
    @Cacheable("users")
    public User getUserOrThrow(Long id) {
        return userRepository.findById(id)
            .orElseThrow(() -> new UserNotFoundException(id));
    }
}

// For negative caching (cache "not found" briefly)
@Service
public class UserService {
    
    private final Cache<Long, Optional<User>> cache;
    
    public Optional<User> getUser(Long id) {
        return cache.get(id, key -> {
            User user = userRepository.findById(key).orElse(null);
            if (user == null) {
                // Cache "not found" for short time only
                // This prevents repeated DB lookups for non-existent users
                return Optional.empty();  // Cached for 1 minute
            }
            return Optional.of(user);  // Cached for 1 hour
        });
    }
    
    @Bean
    public Cache<Long, Optional<User>> userCache() {
        return Caffeine.newBuilder()
            .maximumSize(10_000)
            .expireAfter(new Expiry<Long, Optional<User>>() {
                @Override
                public long expireAfterCreate(Long key, Optional<User> value, long currentTime) {
                    // Short TTL for "not found", long TTL for found
                    return value.isPresent() 
                        ? TimeUnit.HOURS.toNanos(1) 
                        : TimeUnit.MINUTES.toNanos(1);
                }
                // ... other methods
            })
            .build();
    }
}
```

---

## 9ï¸âƒ£ Anti-Pattern #8: Wrong Cache Level

### The Problem

```mermaid
flowchart TD
    Title["WRONG CACHE LEVEL<br/>Scenario: 10 app servers, each with local cache"]
    
    Problem1["User updates profile on Server 1<br/>Server 1 invalidates its local cache<br/>Servers 2-10 still have stale data!"]
    
    Result1["User's next request goes to Server 5<br/>User sees old profile data<br/>User updates again, goes to Server 3<br/>User sees old data AGAIN"]
    
    Note1["This is the cache coherency problem with local caches."]
    
    Divider["â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"]
    
    Problem2["Opposite problem: Using distributed cache for everything"]
    
    Result2["Network latency: 1-5ms per Redis call<br/>10 cache lookups per request = 10-50ms added latency<br/>For read-heavy, rarely-changing data, local cache is faster"]
    
    Title --> Problem1 --> Result1 --> Note1
    Title --> Divider --> Problem2 --> Result2
```

<details>
<summary>ASCII diagram (reference)</summary>

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WRONG CACHE LEVEL                                     â”‚
â”‚                                                                          â”‚
â”‚   Scenario: 10 app servers, each with local cache                       â”‚
â”‚                                                                          â”‚
â”‚   User updates profile on Server 1                                      â”‚
â”‚   Server 1 invalidates its local cache                                  â”‚
â”‚   Servers 2-10 still have stale data!                                   â”‚
â”‚                                                                          â”‚
â”‚   User's next request goes to Server 5                                  â”‚
â”‚   User sees old profile data                                            â”‚
â”‚   User updates again, goes to Server 3                                  â”‚
â”‚   User sees old data AGAIN                                              â”‚
â”‚                                                                          â”‚
â”‚   This is the "cache coherency" problem with local caches.              â”‚
â”‚                                                                          â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                          â”‚
â”‚   Opposite problem: Using distributed cache for everything              â”‚
â”‚                                                                          â”‚
â”‚   Network latency: 1-5ms per Redis call                                 â”‚
â”‚   10 cache lookups per request = 10-50ms added latency                 â”‚
â”‚   For read-heavy, rarely-changing data, local cache is faster          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
</details>

### Good Code

```java
// âœ… CORRECT: Multi-level caching with appropriate levels
@Service
public class ProductService {
    
    private final Cache<Long, Product> localCache;  // L1: Local (Caffeine)
    private final RedisTemplate<String, Product> redis;  // L2: Distributed
    private final ProductRepository repository;  // L3: Database
    
    public Product getProduct(Long id) {
        // L1: Check local cache first (fastest)
        Product product = localCache.getIfPresent(id);
        if (product != null) {
            return product;
        }
        
        // L2: Check distributed cache
        String redisKey = "product:" + id;
        product = redis.opsForValue().get(redisKey);
        if (product != null) {
            // Populate L1 from L2
            localCache.put(id, product);
            return product;
        }
        
        // L3: Fetch from database
        product = repository.findById(id).orElse(null);
        if (product != null) {
            // Populate both caches
            redis.opsForValue().set(redisKey, product, Duration.ofHours(1));
            localCache.put(id, product);
        }
        
        return product;
    }
    
    // Invalidate all levels on update
    public void updateProduct(Long id, ProductUpdate update) {
        // Update database
        Product product = repository.findById(id).orElseThrow();
        product.apply(update);
        repository.save(product);
        
        // Invalidate distributed cache (affects all servers)
        redis.delete("product:" + id);
        
        // Invalidate local cache
        localCache.invalidate(id);
        
        // Publish event for other servers to invalidate their local caches
        eventPublisher.publishEvent(new ProductInvalidatedEvent(id));
    }
}

// Listen for invalidation events from other servers
@Component
public class CacheInvalidationListener {
    
    @Autowired
    private Cache<Long, Product> localCache;
    
    @RedisListener(topics = "cache-invalidation")
    public void onInvalidation(ProductInvalidatedEvent event) {
        localCache.invalidate(event.getProductId());
    }
}
```

---

## ğŸ”Ÿ Summary: Caching Anti-Patterns Checklist

```mermaid
flowchart TD
    Title["CACHING ANTI-PATTERNS CHECKLIST<br/>Before deploying caching, verify:"]
    
    Checklist["â–¡ Invalidation strategy defined for all mutable data<br/>â–¡ Cache size is bounded (won't exhaust memory)<br/>â–¡ Cache keys are unique and namespaced<br/>â–¡ Stampede protection in place for hot keys<br/>â–¡ Monitoring for hit rate, memory, latency<br/>â–¡ Errors and nulls handled appropriately<br/>â–¡ Right cache level (local vs distributed) for each use case<br/>â–¡ TTLs appropriate for data freshness requirements<br/>â–¡ Private data not cached publicly (security)<br/>â–¡ Cache warming strategy for cold starts"]
    
    Remember["Remember:<br/>- Caching is an optimization, not a requirement<br/>- Start without cache, add when you have evidence of need<br/>- Monitor everything - cache without metrics is blind<br/>- Test cache invalidation as thoroughly as cache population"]
    
    Title --> Checklist --> Remember
```

<details>
<summary>ASCII diagram (reference)</summary>

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CACHING ANTI-PATTERNS CHECKLIST                       â”‚
â”‚                                                                          â”‚
â”‚   Before deploying caching, verify:                                      â”‚
â”‚                                                                          â”‚
â”‚   â–¡ Invalidation strategy defined for all mutable data                  â”‚
â”‚   â–¡ Cache size is bounded (won't exhaust memory)                        â”‚
â”‚   â–¡ Cache keys are unique and namespaced                                â”‚
â”‚   â–¡ Stampede protection in place for hot keys                           â”‚
â”‚   â–¡ Monitoring for hit rate, memory, latency                            â”‚
â”‚   â–¡ Errors and nulls handled appropriately                              â”‚
â”‚   â–¡ Right cache level (local vs distributed) for each use case          â”‚
â”‚   â–¡ TTLs appropriate for data freshness requirements                    â”‚
â”‚   â–¡ Private data not cached publicly (security)                         â”‚
â”‚   â–¡ Cache warming strategy for cold starts                              â”‚
â”‚                                                                          â”‚
â”‚   Remember:                                                              â”‚
â”‚   - Caching is an optimization, not a requirement                       â”‚
â”‚   - Start without cache, add when you have evidence of need             â”‚
â”‚   - Monitor everything - cache without metrics is blind                 â”‚
â”‚   - Test cache invalidation as thoroughly as cache population           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
</details>

---

## Interview Follow-Up Questions WITH Answers

### L4 Questions

**Q: What's a common caching mistake you've seen?**

A: The most common mistake is caching mutable data without invalidation. Developers add `@Cacheable` to speed up reads but forget to add `@CacheEvict` on updates. This leads to stale data being served to users. For example, a product price gets updated in the database but users see the old cached price for hours. The fix is simple: always pair cache reads with cache invalidation on writes, or use event-driven invalidation.

### L5 Questions

**Q: How do you decide what to cache and what not to cache?**

A: I use a decision framework based on four factors: (1) Access frequency - cache data accessed multiple times per second, not data accessed once per day. (2) Computation cost - cache expensive queries and API calls, not simple primary key lookups. (3) Change frequency - cache rarely-changing data with long TTLs, frequently-changing data needs careful invalidation. (4) Staleness tolerance - cache data where slightly stale is acceptable (recommendations), be careful with data requiring freshness (inventory, prices). I also monitor hit rates after deployment - if hit rate is below 80%, the cache might not be worth the complexity.

### L6 Questions

**Q: Design a caching strategy that avoids all the common anti-patterns.**

A: I'd implement a multi-level caching architecture with these safeguards:

**Cache selection**: L1 local cache (Caffeine) for hot, read-heavy data. L2 distributed cache (Redis) for shared state. Clear criteria for what goes where.

**Invalidation**: Event-driven using Kafka. When data changes, publish event. All services subscribe and invalidate their caches. Use surrogate keys for related data.

**Stampede protection**: Distributed locks with Redis for cache population. Stale-while-revalidate for non-critical data.

**Key design**: Namespace all keys (`product:123`, `user:456`). Use consistent key generation across services.

**Monitoring**: Prometheus metrics for hit rate, memory, latency. Alerts when hit rate drops below 70% or memory exceeds 80%.

**Error handling**: Never cache errors. Short TTL for "not found" (negative caching). Circuit breaker for cache failures.

**Bounded size**: All caches have maximum size limits. Eviction policies based on access patterns (LRU for most, LFU for skewed access).

This approach addresses each anti-pattern systematically while maintaining performance and reliability.

