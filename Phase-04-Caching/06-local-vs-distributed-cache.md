# ğŸ  Local vs Distributed Cache

---

## 0ï¸âƒ£ Prerequisites

Before diving into local vs distributed caching, you need to understand:

- **Cache**: A fast storage layer for frequently accessed data. Covered in Topics 1-5.
- **JVM Heap**: The memory area where Java objects live. Limited by `-Xmx` setting.
- **Network Latency**: Time for data to travel between machines. Even within a data center, this is ~0.5-2ms.
- **Serialization**: Converting objects to bytes for transmission. Required for distributed caches.

If you understand that accessing local memory is faster than network calls, you're ready.

---

## 1ï¸âƒ£ What Problem Does This Exist to Solve?

### The Pain Point

You're building a product catalog service. You have two options:

**Option A: Local Cache (HashMap in JVM)**
```java
private Map<Long, Product> cache = new ConcurrentHashMap<>();
```

**Option B: Distributed Cache (Redis)**
```java
redisTemplate.opsForValue().get("product:" + id);
```

Which should you choose? The answer is: **it depends**.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE TRADEOFF                                          â”‚
â”‚                                                                          â”‚
â”‚   LOCAL CACHE                          DISTRIBUTED CACHE                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚   â”‚  âš¡ 50 nanoseconds  â”‚              â”‚  ğŸŒ 0.5-2 millisecondsâ”‚         â”‚
â”‚   â”‚  Access time        â”‚              â”‚  Access time         â”‚          â”‚
â”‚   â”‚                     â”‚              â”‚                      â”‚          â”‚
â”‚   â”‚  âŒ Not shared      â”‚              â”‚  âœ… Shared across    â”‚          â”‚
â”‚   â”‚  across servers     â”‚              â”‚  all servers         â”‚          â”‚
â”‚   â”‚                     â”‚              â”‚                      â”‚          â”‚
â”‚   â”‚  âŒ Lost on restart â”‚              â”‚  âœ… Survives restartsâ”‚          â”‚
â”‚   â”‚                     â”‚              â”‚                      â”‚          â”‚
â”‚   â”‚  âŒ Limited to JVM  â”‚              â”‚  âœ… Scales to TBs    â”‚          â”‚
â”‚   â”‚  heap size          â”‚              â”‚                      â”‚          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                          â”‚
â”‚   Local is 10,000-40,000x faster but has significant limitations        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What Breaks with Wrong Choice?

**Using Only Local Cache**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LOCAL CACHE PROBLEM                                   â”‚
â”‚                                                                          â”‚
â”‚   Server 1                    Server 2                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚   â”‚ Cache:           â”‚       â”‚ Cache:           â”‚                       â”‚
â”‚   â”‚ product:123 =    â”‚       â”‚ product:123 =    â”‚                       â”‚
â”‚   â”‚ {price: $99}     â”‚       â”‚ {price: $79}     â”‚  â† Different values!  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                                          â”‚
â”‚   User A sees $99            User B sees $79                            â”‚
â”‚   (Server 1)                 (Server 2)                                 â”‚
â”‚                                                                          â”‚
â”‚   Problems:                                                              â”‚
â”‚   1. Inconsistent data across servers                                   â”‚
â”‚   2. Cache wasted - same data cached N times on N servers              â”‚
â”‚   3. Cold start after deployment - all caches empty                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Using Only Distributed Cache**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DISTRIBUTED CACHE PROBLEM                             â”‚
â”‚                                                                          â”‚
â”‚   Server 1     Server 2     Server 3                                    â”‚
â”‚      â”‚            â”‚            â”‚                                        â”‚
â”‚      â”‚  1ms       â”‚  1ms       â”‚  1ms                                   â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚                   â”‚                                                      â”‚
â”‚                   â–¼                                                      â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                             â”‚
â”‚            â”‚    Redis     â”‚                                             â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â”‚
â”‚                                                                          â”‚
â”‚   10,000 requests/second Ã— 3 servers = 30,000 Redis calls/second       â”‚
â”‚                                                                          â”‚
â”‚   Problems:                                                              â”‚
â”‚   1. Network latency on every access (1ms vs 50ns)                     â”‚
â”‚   2. Redis becomes bottleneck                                           â”‚
â”‚   3. Network bandwidth consumed                                         â”‚
â”‚   4. Single point of failure                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Real Examples

**Netflix**: Uses a two-level cache. L1 (local, Guava/Caffeine) for extremely hot data like configuration. L2 (EVCache/Memcached) for shared data like user profiles.

**Facebook**: Uses local caches in front of Memcached for the hottest keys. A single popular user's profile might be accessed millions of times per second.

**Uber**: Uses local caches for surge pricing calculations (needs sub-millisecond latency) and distributed cache for driver/rider matching data.

---

## 2ï¸âƒ£ Intuition and Mental Model

### The Office Supplies Analogy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE OFFICE SUPPLIES ANALOGY                           â”‚
â”‚                                                                          â”‚
â”‚   LOCAL CACHE = Your Desk Drawer                                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                                                                  â”‚   â”‚
â”‚   â”‚   âœ… Instant access (reach into drawer)                         â”‚   â”‚
â”‚   â”‚   âœ… No one else can take your stuff                            â”‚   â”‚
â”‚   â”‚   âŒ Limited space (one small drawer)                           â”‚   â”‚
â”‚   â”‚   âŒ Coworker can't borrow your stapler                         â”‚   â”‚
â”‚   â”‚   âŒ If you change desks, drawer is empty                       â”‚   â”‚
â”‚   â”‚                                                                  â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                          â”‚
â”‚   DISTRIBUTED CACHE = Supply Closet Down the Hall                       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                                                                  â”‚   â”‚
â”‚   â”‚   âœ… Everyone can access it                                      â”‚   â”‚
â”‚   â”‚   âœ… Huge storage capacity                                       â”‚   â”‚
â”‚   â”‚   âœ… Survives if you change desks                               â”‚   â”‚
â”‚   â”‚   âŒ Takes 30 seconds to walk there                             â”‚   â”‚
â”‚   â”‚   âŒ Might be crowded (contention)                              â”‚   â”‚
â”‚   â”‚                                                                  â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                          â”‚
â”‚   HYBRID = Keep frequently used items in drawer,                        â”‚
â”‚            get less common items from closet                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3ï¸âƒ£ How It Works Internally

### Local Cache Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LOCAL CACHE (Caffeine)                                â”‚
â”‚                                                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                        JVM HEAP                                  â”‚   â”‚
â”‚   â”‚                                                                  â”‚   â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚   â”‚
â”‚   â”‚   â”‚                   CAFFEINE CACHE                       â”‚     â”‚   â”‚
â”‚   â”‚   â”‚                                                        â”‚     â”‚   â”‚
â”‚   â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚   â”‚
â”‚   â”‚   â”‚   â”‚   Window    â”‚  â”‚  Probation  â”‚  â”‚  Protected  â”‚  â”‚     â”‚   â”‚
â”‚   â”‚   â”‚   â”‚   (1%)      â”‚  â”‚   (20%)     â”‚  â”‚   (79%)     â”‚  â”‚     â”‚   â”‚
â”‚   â”‚   â”‚   â”‚             â”‚  â”‚             â”‚  â”‚             â”‚  â”‚     â”‚   â”‚
â”‚   â”‚   â”‚   â”‚  New items  â”‚  â”‚  Promotion  â”‚  â”‚ Frequently  â”‚  â”‚     â”‚   â”‚
â”‚   â”‚   â”‚   â”‚  enter here â”‚â”€â”€â–¶â”‚  candidates â”‚â”€â”€â–¶â”‚ accessed    â”‚  â”‚     â”‚   â”‚
â”‚   â”‚   â”‚   â”‚             â”‚  â”‚             â”‚  â”‚             â”‚  â”‚     â”‚   â”‚
â”‚   â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚   â”‚
â”‚   â”‚   â”‚                                                        â”‚     â”‚   â”‚
â”‚   â”‚   â”‚   Uses W-TinyLFU algorithm (combines LRU + LFU)       â”‚     â”‚   â”‚
â”‚   â”‚   â”‚   Near-optimal hit rate                                â”‚     â”‚   â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚   â”‚
â”‚   â”‚                                                                  â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                          â”‚
â”‚   Access Time: ~50 nanoseconds (direct memory access)                   â”‚
â”‚   No serialization needed                                                â”‚
â”‚   No network call                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Distributed Cache Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DISTRIBUTED CACHE (Redis)                             â”‚
â”‚                                                                          â”‚
â”‚   Application Server                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  1. Serialize object to JSON/bytes                               â”‚   â”‚
â”‚   â”‚  2. Send over TCP to Redis                                       â”‚   â”‚
â”‚   â”‚  3. Wait for response                                            â”‚   â”‚
â”‚   â”‚  4. Deserialize response                                         â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                   â”‚                                      â”‚
â”‚                                   â”‚ TCP (1-2ms round trip)              â”‚
â”‚                                   â”‚                                      â”‚
â”‚                                   â–¼                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                         REDIS SERVER                             â”‚   â”‚
â”‚   â”‚                                                                  â”‚   â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚   â”‚   â”‚                    MEMORY                                â”‚   â”‚   â”‚
â”‚   â”‚   â”‚                                                          â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   Hash Table with data                                   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   "product:123" â†’ "{name: 'Keyboard', price: 99.99}"   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚                                                          â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚   â”‚                                                                  â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                          â”‚
â”‚   Access Time: ~500-2000 microseconds (network + serialization)         â”‚
â”‚   Shared across all application servers                                  â”‚
â”‚   Survives application restarts                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Hybrid (Multi-Level) Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MULTI-LEVEL CACHE (L1 + L2)                           â”‚
â”‚                                                                          â”‚
â”‚   Request Flow:                                                          â”‚
â”‚                                                                          â”‚
â”‚   1. Check L1 (local)                                                    â”‚
â”‚      â”‚                                                                   â”‚
â”‚      â”œâ”€â”€ HIT â†’ Return immediately (50ns)                                â”‚
â”‚      â”‚                                                                   â”‚
â”‚      â””â”€â”€ MISS â†’ Check L2 (distributed)                                  â”‚
â”‚                 â”‚                                                        â”‚
â”‚                 â”œâ”€â”€ HIT â†’ Store in L1, return (1-2ms)                   â”‚
â”‚                 â”‚                                                        â”‚
â”‚                 â””â”€â”€ MISS â†’ Query database                               â”‚
â”‚                            â”‚                                             â”‚
â”‚                            â””â”€â”€ Store in L2, store in L1, return         â”‚
â”‚                                                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  Server 1          Server 2          Server 3                    â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚   â”‚
â”‚   â”‚  â”‚ L1 Cache â”‚     â”‚ L1 Cache â”‚     â”‚ L1 Cache â”‚                 â”‚   â”‚
â”‚   â”‚  â”‚ (local)  â”‚     â”‚ (local)  â”‚     â”‚ (local)  â”‚                 â”‚   â”‚
â”‚   â”‚  â”‚ 1000     â”‚     â”‚ 1000     â”‚     â”‚ 1000     â”‚                 â”‚   â”‚
â”‚   â”‚  â”‚ items    â”‚     â”‚ items    â”‚     â”‚ items    â”‚                 â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                 â”‚   â”‚
â”‚   â”‚       â”‚                â”‚                â”‚                        â”‚   â”‚
â”‚   â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚   â”‚
â”‚   â”‚                        â”‚                                         â”‚   â”‚
â”‚   â”‚                        â–¼                                         â”‚   â”‚
â”‚   â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚   â”‚
â”‚   â”‚            â”‚      L2 Cache         â”‚                            â”‚   â”‚
â”‚   â”‚            â”‚      (Redis)          â”‚                            â”‚   â”‚
â”‚   â”‚            â”‚   1,000,000 items     â”‚                            â”‚   â”‚
â”‚   â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚   â”‚
â”‚   â”‚                        â”‚                                         â”‚   â”‚
â”‚   â”‚                        â–¼                                         â”‚   â”‚
â”‚   â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚   â”‚
â”‚   â”‚            â”‚      Database         â”‚                            â”‚   â”‚
â”‚   â”‚            â”‚  100,000,000 items    â”‚                            â”‚   â”‚
â”‚   â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4ï¸âƒ£ Simulation-First Explanation

### Scenario: Product Catalog with 10,000 requests/second

**Setup**:
- 3 application servers
- 1 million products
- 80% of requests hit 1% of products (hot items)

### Local Only

```
Request for product:123 on Server 1:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Time: 0ns     Check local cache
Time: 50ns    Cache HIT! Return product
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: 50 nanoseconds

Same request on Server 2 (first time):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Time: 0ns     Check local cache
Time: 50ns    Cache MISS
Time: 50ns    Query database
Time: 50ms    Database returns
Time: 50ms    Store in local cache
Time: 50ms    Return product
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: 50 milliseconds (1000x slower for first access on each server!)

Problem: 3 servers Ã— 10,000 products = potential 30,000 database queries
         instead of 10,000
```

### Distributed Only

```
Request for product:123 on any server:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Time: 0ms       Serialize request
Time: 0.1ms     Send to Redis
Time: 0.5ms     Network latency
Time: 0.6ms     Redis processes
Time: 0.7ms     Network latency (return)
Time: 0.8ms     Deserialize response
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: ~1 millisecond

10,000 requests/second Ã— 1ms = 10 seconds of cumulative latency/second
Redis handling 10,000 requests/second (approaching limits)
```

### Hybrid (L1 + L2)

```
Request for HOT product:123:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Time: 0ns     Check L1 (local cache)
Time: 50ns    L1 HIT! Return immediately
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: 50 nanoseconds

Request for COLD product:999999 (first time on this server):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Time: 0ns       Check L1 (local cache) - MISS
Time: 0.1ms     Check L2 (Redis)
Time: 1ms       L2 HIT! (another server cached it earlier)
Time: 1ms       Store in L1
Time: 1ms       Return product
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: ~1 millisecond (but subsequent requests: 50ns)

Result:
- 80% of requests (hot items): 50ns (L1 hit)
- 19% of requests (warm items): 1ms (L2 hit)
- 1% of requests (cold items): 50ms (database)

Average latency: 0.8 Ã— 0.00005ms + 0.19 Ã— 1ms + 0.01 Ã— 50ms = 0.69ms
vs Local only: Higher database load
vs Distributed only: 1ms average
```

---

## 5ï¸âƒ£ How Engineers Actually Use This in Production

### Netflix's Caching Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NETFLIX CACHING LAYERS                                â”‚
â”‚                                                                          â”‚
â”‚   Layer 0: In-Process Cache (Guava/Caffeine)                            â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”‚
â”‚   - Config values, feature flags                                         â”‚
â”‚   - TTL: Minutes                                                         â”‚
â”‚   - Size: ~100MB per instance                                           â”‚
â”‚                                                                          â”‚
â”‚   Layer 1: EVCache (Memcached-based)                                    â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”‚
â”‚   - User profiles, viewing history                                       â”‚
â”‚   - TTL: Hours                                                           â”‚
â”‚   - Size: Terabytes                                                      â”‚
â”‚   - Cross-region replication                                             â”‚
â”‚                                                                          â”‚
â”‚   Layer 2: Cassandra                                                     â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”‚
â”‚   - Persistent storage                                                   â”‚
â”‚   - Source of truth                                                      â”‚
â”‚                                                                          â”‚
â”‚   Special: Hollow (local read-only datasets)                            â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”‚
â”‚   - Movie catalog (changes infrequently)                                â”‚
â”‚   - Loaded entirely into memory                                          â”‚
â”‚   - No cache misses!                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Facebook's TAO

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FACEBOOK TAO ARCHITECTURE                             â”‚
â”‚                                                                          â”‚
â”‚   TAO = The Associations and Objects cache                              â”‚
â”‚                                                                          â”‚
â”‚   Request Flow:                                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                                                                  â”‚   â”‚
â”‚   â”‚   Web Server                                                     â”‚   â”‚
â”‚   â”‚       â”‚                                                          â”‚   â”‚
â”‚   â”‚       â–¼                                                          â”‚   â”‚
â”‚   â”‚   TAO Leader (in-region cache)                                  â”‚   â”‚
â”‚   â”‚       â”‚                                                          â”‚   â”‚
â”‚   â”‚       â”œâ”€â”€ HIT â†’ Return (99%+ of requests)                       â”‚   â”‚
â”‚   â”‚       â”‚                                                          â”‚   â”‚
â”‚   â”‚       â””â”€â”€ MISS â†’ TAO Follower (other region)                    â”‚   â”‚
â”‚   â”‚                   â”‚                                              â”‚   â”‚
â”‚   â”‚                   â”œâ”€â”€ HIT â†’ Return + async populate leader      â”‚   â”‚
â”‚   â”‚                   â”‚                                              â”‚   â”‚
â”‚   â”‚                   â””â”€â”€ MISS â†’ MySQL (rare)                       â”‚   â”‚
â”‚   â”‚                                                                  â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                          â”‚
â”‚   Key insight: Leader caches HOT data for that region                   â”‚
â”‚                Follower acts as L2 with global data                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 6ï¸âƒ£ How to Implement in Java

### Caffeine (Local Cache)

```java
// CaffeineLocalCache.java
package com.example.cache.local;

import com.github.benmanes.caffeine.cache.Cache;
import com.github.benmanes.caffeine.cache.Caffeine;
import com.github.benmanes.caffeine.cache.LoadingCache;
import com.github.benmanes.caffeine.cache.stats.CacheStats;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Component;

import java.time.Duration;
import java.util.concurrent.CompletableFuture;
import java.util.function.Function;

/**
 * Local cache implementation using Caffeine
 * 
 * Caffeine is the successor to Guava Cache with better performance
 * Uses Window TinyLFU eviction policy for near-optimal hit rates
 */
@Component
@Slf4j
public class CaffeineLocalCache<K, V> {

    private final Cache<K, V> cache;

    public CaffeineLocalCache() {
        this.cache = Caffeine.newBuilder()
            // Maximum entries in cache
            .maximumSize(10_000)
            
            // Expire after write (fixed TTL)
            .expireAfterWrite(Duration.ofMinutes(5))
            
            // Expire after last access (sliding TTL)
            // .expireAfterAccess(Duration.ofMinutes(10))
            
            // Enable statistics for monitoring
            .recordStats()
            
            // Listener for evictions (useful for debugging)
            .evictionListener((key, value, cause) -> 
                log.debug("Evicted {} due to {}", key, cause))
            
            // Build the cache
            .build();
    }

    /**
     * Get value, return null if not present
     */
    public V get(K key) {
        return cache.getIfPresent(key);
    }

    /**
     * Get value, compute if absent
     * This is atomic - only one computation for concurrent requests
     */
    public V get(K key, Function<K, V> loader) {
        return cache.get(key, loader);
    }

    /**
     * Put value in cache
     */
    public void put(K key, V value) {
        cache.put(key, value);
    }

    /**
     * Invalidate specific key
     */
    public void invalidate(K key) {
        cache.invalidate(key);
    }

    /**
     * Invalidate all keys
     */
    public void invalidateAll() {
        cache.invalidateAll();
    }

    /**
     * Get cache statistics
     */
    public CacheStats stats() {
        return cache.stats();
    }

    /**
     * Print statistics for monitoring
     */
    public void printStats() {
        CacheStats stats = cache.stats();
        log.info("""
            Cache Statistics:
            - Hit Rate: {:.2f}%
            - Miss Rate: {:.2f}%
            - Load Count: {}
            - Eviction Count: {}
            - Average Load Time: {:.2f}ms
            """,
            stats.hitRate() * 100,
            stats.missRate() * 100,
            stats.loadCount(),
            stats.evictionCount(),
            stats.averageLoadPenalty() / 1_000_000.0
        );
    }
}
```

### Multi-Level Cache (L1 Local + L2 Redis)

```java
// MultiLevelCache.java
package com.example.cache.multilevel;

import com.github.benmanes.caffeine.cache.Cache;
import com.github.benmanes.caffeine.cache.Caffeine;
import lombok.extern.slf4j.Slf4j;
import org.springframework.data.redis.core.RedisTemplate;
import org.springframework.stereotype.Service;

import java.time.Duration;
import java.util.Optional;
import java.util.function.Supplier;

/**
 * Multi-level cache: L1 (Caffeine local) + L2 (Redis distributed)
 * 
 * L1 provides sub-millisecond access for hot data
 * L2 provides shared cache across all servers
 */
@Service
@Slf4j
public class MultiLevelCache<V> {

    private final Cache<String, V> l1Cache;
    private final RedisTemplate<String, V> redisTemplate;
    
    // Configuration
    private static final int L1_MAX_SIZE = 10_000;
    private static final Duration L1_TTL = Duration.ofMinutes(1);
    private static final Duration L2_TTL = Duration.ofMinutes(30);

    public MultiLevelCache(RedisTemplate<String, V> redisTemplate) {
        this.redisTemplate = redisTemplate;
        
        // Initialize L1 (local cache)
        this.l1Cache = Caffeine.newBuilder()
            .maximumSize(L1_MAX_SIZE)
            .expireAfterWrite(L1_TTL)
            .recordStats()
            .build();
    }

    /**
     * Get from multi-level cache
     * 
     * 1. Check L1 (local)
     * 2. Check L2 (Redis)
     * 3. Load from source
     */
    public V get(String key, Supplier<V> loader) {
        // Step 1: Check L1 (local cache)
        V value = l1Cache.getIfPresent(key);
        if (value != null) {
            log.debug("L1 HIT for {}", key);
            return value;
        }
        
        // Step 2: Check L2 (Redis)
        value = redisTemplate.opsForValue().get(key);
        if (value != null) {
            log.debug("L2 HIT for {}", key);
            // Promote to L1
            l1Cache.put(key, value);
            return value;
        }
        
        // Step 3: Load from source (database)
        log.debug("MISS for {}, loading from source", key);
        value = loader.get();
        
        if (value != null) {
            // Store in both L1 and L2
            l1Cache.put(key, value);
            redisTemplate.opsForValue().set(key, value, L2_TTL);
        }
        
        return value;
    }

    /**
     * Invalidate from all levels
     */
    public void invalidate(String key) {
        l1Cache.invalidate(key);
        redisTemplate.delete(key);
        log.debug("Invalidated {} from all cache levels", key);
    }

    /**
     * Invalidate L1 only (for distributed invalidation)
     * Called when receiving invalidation message from other servers
     */
    public void invalidateL1(String key) {
        l1Cache.invalidate(key);
        log.debug("Invalidated {} from L1", key);
    }

    /**
     * Put directly (bypassing load)
     */
    public void put(String key, V value) {
        l1Cache.put(key, value);
        redisTemplate.opsForValue().set(key, value, L2_TTL);
    }

    /**
     * Get L1 stats for monitoring
     */
    public String getL1Stats() {
        var stats = l1Cache.stats();
        return String.format("L1 Hit Rate: %.2f%%, Size: %d", 
            stats.hitRate() * 100, l1Cache.estimatedSize());
    }
}
```

### L1 Invalidation via Redis Pub/Sub

```java
// CacheInvalidationBroadcaster.java
package com.example.cache.multilevel;

import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.data.redis.core.RedisTemplate;
import org.springframework.data.redis.listener.ChannelTopic;
import org.springframework.stereotype.Service;

/**
 * Broadcasts cache invalidation messages to all servers
 * 
 * When one server invalidates a cache entry, all other servers
 * need to invalidate their L1 caches too.
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class CacheInvalidationBroadcaster {

    private final RedisTemplate<String, String> redisTemplate;
    
    private static final String INVALIDATION_CHANNEL = "cache:invalidation";

    /**
     * Broadcast invalidation to all servers
     */
    public void broadcastInvalidation(String cacheKey) {
        log.debug("Broadcasting invalidation for {}", cacheKey);
        redisTemplate.convertAndSend(INVALIDATION_CHANNEL, cacheKey);
    }
}
```

```java
// CacheInvalidationListener.java
package com.example.cache.multilevel;

import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.data.redis.connection.Message;
import org.springframework.data.redis.connection.MessageListener;
import org.springframework.stereotype.Component;

/**
 * Listens for cache invalidation messages from other servers
 */
@Component
@RequiredArgsConstructor
@Slf4j
public class CacheInvalidationListener implements MessageListener {

    private final MultiLevelCache<?> multiLevelCache;

    @Override
    public void onMessage(Message message, byte[] pattern) {
        String cacheKey = new String(message.getBody());
        log.debug("Received invalidation for {}", cacheKey);
        
        // Only invalidate L1 - L2 (Redis) is already updated
        multiLevelCache.invalidateL1(cacheKey);
    }
}
```

```java
// RedisListenerConfig.java
package com.example.cache.config;

import com.example.cache.multilevel.CacheInvalidationListener;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.data.redis.connection.RedisConnectionFactory;
import org.springframework.data.redis.listener.ChannelTopic;
import org.springframework.data.redis.listener.RedisMessageListenerContainer;

@Configuration
public class RedisListenerConfig {

    @Bean
    public RedisMessageListenerContainer redisContainer(
            RedisConnectionFactory connectionFactory,
            CacheInvalidationListener listener) {
        
        RedisMessageListenerContainer container = new RedisMessageListenerContainer();
        container.setConnectionFactory(connectionFactory);
        container.addMessageListener(listener, new ChannelTopic("cache:invalidation"));
        return container;
    }
}
```

### Complete Product Service Example

```java
// ProductService.java
package com.example.cache.service;

import com.example.cache.multilevel.CacheInvalidationBroadcaster;
import com.example.cache.multilevel.MultiLevelCache;
import com.example.domain.Product;
import com.example.repository.ProductRepository;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Service;

@Service
@RequiredArgsConstructor
@Slf4j
public class ProductService {

    private final MultiLevelCache<Product> cache;
    private final CacheInvalidationBroadcaster broadcaster;
    private final ProductRepository productRepository;

    private static final String KEY_PREFIX = "product:";

    /**
     * Get product with multi-level caching
     */
    public Product getProduct(Long productId) {
        String key = KEY_PREFIX + productId;
        
        return cache.get(key, () -> {
            log.info("Loading product {} from database", productId);
            return productRepository.findById(productId).orElse(null);
        });
    }

    /**
     * Update product - invalidate cache across all servers
     */
    public Product updateProduct(Long productId, Product updates) {
        Product saved = productRepository.save(updates);
        
        String key = KEY_PREFIX + productId;
        
        // Invalidate local cache and Redis
        cache.invalidate(key);
        
        // Broadcast to other servers to invalidate their L1
        broadcaster.broadcastInvalidation(key);
        
        return saved;
    }

    /**
     * Delete product - invalidate cache
     */
    public void deleteProduct(Long productId) {
        productRepository.deleteById(productId);
        
        String key = KEY_PREFIX + productId;
        cache.invalidate(key);
        broadcaster.broadcastInvalidation(key);
    }
}
```

---

## 7ï¸âƒ£ Tradeoffs, Pitfalls, and Common Mistakes

### Comparison Table

| Aspect | Local Cache | Distributed Cache | Multi-Level |
|--------|-------------|-------------------|-------------|
| **Latency** | ~50ns | ~1ms | 50ns (L1 hit) to 1ms |
| **Consistency** | Weak | Strong | Eventual |
| **Capacity** | Limited (heap) | Large (TB) | Both |
| **Failure Impact** | None | High | Medium |
| **Complexity** | Low | Medium | High |
| **Best For** | Hot data, config | Shared data | High-scale systems |

### Common Mistakes

**1. L1 TTL Too Long**

```java
// WRONG: L1 TTL same as L2
.expireAfterWrite(Duration.ofMinutes(30))  // Too long for L1!

// RIGHT: L1 should be much shorter
.expireAfterWrite(Duration.ofMinutes(1))   // Quick turnover
```

**2. Not Handling L1 Invalidation**

```java
// WRONG: Only invalidate Redis
public void updateProduct(Product product) {
    repository.save(product);
    redisTemplate.delete("product:" + product.getId());
    // L1 on other servers still has old data!
}

// RIGHT: Broadcast invalidation
public void updateProduct(Product product) {
    repository.save(product);
    cache.invalidate("product:" + product.getId());
    broadcaster.broadcastInvalidation("product:" + product.getId());
}
```

**3. L1 Too Large**

```java
// WRONG: L1 eats all your heap
.maximumSize(10_000_000)  // 10 million items = potential OOM

// RIGHT: Size L1 appropriately
.maximumSize(10_000)      // Keep it small, let L2 handle the rest
// Or use weight-based sizing
.maximumWeight(100_000_000)  // 100MB max
.weigher((k, v) -> estimateSize(v))
```

---

## 8ï¸âƒ£ When NOT to Use Each Type

### Local Cache: Don't Use When

- Data must be consistent across servers immediately
- Cache size would exceed available heap
- Data is rarely accessed (wasted memory on each server)

### Distributed Cache: Don't Use When

- Sub-millisecond latency is required
- Network reliability is poor
- Data is server-specific (not shared)

### Multi-Level: Don't Use When

- System is simple (single server)
- Eventual consistency is unacceptable
- Operational complexity is a concern

---

## 9ï¸âƒ£ Interview Follow-Up Questions WITH Answers

### L4 Questions

**Q: What's the difference between local and distributed cache?**

A: Local cache (like Caffeine) stores data in the application's memory (JVM heap). Access is extremely fast (~50 nanoseconds) but data isn't shared between servers. Distributed cache (like Redis) stores data on separate cache servers. Access is slower (~1 millisecond) due to network latency, but data is shared across all application servers. Local is best for hot data where speed is critical; distributed is best for shared state and large datasets.

### L5 Questions

**Q: How do you keep L1 caches synchronized across servers?**

A: I use pub/sub messaging. When one server updates data, it publishes an invalidation message to a Redis channel. All other servers subscribe to this channel and invalidate their L1 caches when they receive the message. This provides eventual consistency. For stronger consistency, I could use shorter L1 TTLs or version-based invalidation where the cache key includes a version number.

### L6 Questions

**Q: Design a caching strategy for a social media feed with 100M users.**

A: I'd use a three-level approach: (1) L1 local cache for the current user's feed and hot celebrity profiles with 30-second TTL. (2) L2 regional Redis cluster for user profiles and recent posts. (3) L3 global Redis for less frequently accessed data. For feed generation, I'd pre-compute feeds for active users and store in Redis. For inactive users, compute on-demand. Hot keys (celebrities) would be replicated across Redis nodes. I'd use pub/sub for L1 invalidation and implement circuit breakers to handle Redis failures gracefully.

---

## ğŸ”Ÿ One Clean Mental Summary

Local caches (Caffeine) provide sub-millisecond access but aren't shared between servers. Distributed caches (Redis) are shared but add network latency. Multi-level caching combines both: L1 local cache handles hot data with instant access, L2 distributed cache handles shared data. The key challenge with multi-level caching is keeping L1 caches synchronized. Use pub/sub or short TTLs for L1 invalidation. Size L1 small (thousands of items), let L2 handle the bulk. Monitor hit rates at each level to tune effectively.

