# Scalability Patterns

## 0ï¸âƒ£ Prerequisites

Before diving into scalability patterns, you should understand:

- **Distributed Systems Basics**: Replication, partitioning, and consistency (covered in Phases 1 and 5)
- **Database Fundamentals**: SQL vs NoSQL, indexing, and sharding (covered in Phase 3)
- **Caching Concepts**: Caching patterns and eviction policies (covered in Phase 4)
- **Trade-off Analysis**: How to evaluate and justify design decisions (covered in Topic 9)

Quick refresher: Scalability is the ability of a system to handle increased load by adding resources. A scalable system can grow to meet demand without fundamental redesign. Understanding scalability patterns is essential for system design interviews because interviewers often ask "How would you scale this to 10x or 100x?"

---

## 1ï¸âƒ£ What Problem Does This Exist to Solve?

### The Specific Pain Point

Systems that work perfectly for 1,000 users often break at 1,000,000 users. The patterns that work at small scale don't work at large scale:

- A single database can handle 10,000 QPS but not 1,000,000 QPS
- A single server can serve 1,000 concurrent users but not 100,000
- Synchronous processing works for 100 requests/second but not 10,000
- A monolithic application works for a 10-person team but not 100

Without understanding scalability patterns, candidates:

1. **Design systems that can't grow**: Single points of failure, no horizontal scaling
2. **Over-engineer for small scale**: Microservices for 100 users
3. **Can't answer "How would you scale this?"**: Common interview question
4. **Miss bottlenecks**: Don't identify what will break first
5. **Propose impractical solutions**: "Just add more servers" without understanding the implications

### What Breaks Without Scalability Thinking

**Scenario 1: The Database Bottleneck**

A candidate designed a system with a single PostgreSQL database. When asked "What if we have 100x more users?", they said "We'll add more application servers." But the database was the bottleneck, and adding app servers wouldn't help.

**Scenario 2: The Synchronous Trap**

A candidate designed an e-commerce checkout that synchronously called 5 external services. At 100 orders/minute, this worked. At 10,000 orders/minute, the p99 latency was 30 seconds because any slow service blocked the entire flow.

**Scenario 3: The State Problem**

A candidate stored user sessions in application server memory. When asked about scaling, they realized they couldn't add more servers without losing session state or implementing sticky sessions (which creates other problems).

---

## 2ï¸âƒ£ Intuition and Mental Model

### The Scaling Ladder

Think of scaling as climbing a ladder. Each rung represents a new level of scale and requires different patterns:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE SCALING LADDER                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  RUNG 5: GLOBAL SCALE (100M+ users)                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                  â”‚
â”‚  - Multi-region deployment                                          â”‚
â”‚  - Global load balancing                                            â”‚
â”‚  - Data sovereignty compliance                                      â”‚
â”‚  - Edge computing                                                   â”‚
â”‚                                                                      â”‚
â”‚  RUNG 4: MASSIVE SCALE (10M+ users)                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                   â”‚
â”‚  - Database sharding                                                â”‚
â”‚  - Event-driven architecture                                        â”‚
â”‚  - Microservices                                                    â”‚
â”‚  - Advanced caching (multi-tier)                                    â”‚
â”‚                                                                      â”‚
â”‚  RUNG 3: LARGE SCALE (1M+ users)                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                    â”‚
â”‚  - Read replicas                                                    â”‚
â”‚  - Message queues                                                   â”‚
â”‚  - CDN for static content                                           â”‚
â”‚  - Distributed caching                                              â”‚
â”‚                                                                      â”‚
â”‚  RUNG 2: MEDIUM SCALE (100K+ users)                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                  â”‚
â”‚  - Load balancing                                                   â”‚
â”‚  - Database optimization (indexes)                                  â”‚
â”‚  - Basic caching                                                    â”‚
â”‚  - Async processing for non-critical paths                          â”‚
â”‚                                                                      â”‚
â”‚  RUNG 1: SMALL SCALE (10K users)                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                    â”‚
â”‚  - Single server                                                    â”‚
â”‚  - Single database                                                  â”‚
â”‚  - Vertical scaling                                                 â”‚
â”‚  - Simple architecture                                              â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Bottleneck Mindset

Scaling is about identifying and removing bottlenecks. The bottleneck is always somewhere:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COMMON BOTTLENECKS                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  LAYER              BOTTLENECK              SOLUTION                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”‚
â”‚  Network            Bandwidth               CDN, compression        â”‚
â”‚  Load Balancer      Connection limits       Multiple LBs            â”‚
â”‚  Application        CPU/Memory              Horizontal scaling      â”‚
â”‚  Database (reads)   Query throughput        Read replicas, cache    â”‚
â”‚  Database (writes)  Write throughput        Sharding, async writes  â”‚
â”‚  Storage            IOPS                    SSD, distributed storageâ”‚
â”‚  External APIs      Rate limits             Caching, batching       â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3ï¸âƒ£ How It Works Internally

### Pattern 1: Horizontal vs Vertical Scaling

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VERTICAL VS HORIZONTAL SCALING                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  VERTICAL SCALING (Scale Up)                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                        â”‚
â”‚                                                                      â”‚
â”‚  Before:          After:                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚  â”‚ 4 CPU   â”‚      â”‚     16 CPU      â”‚                               â”‚
â”‚  â”‚ 16 GB   â”‚  â†’   â”‚     64 GB       â”‚                               â”‚
â”‚  â”‚ 500 GB  â”‚      â”‚     2 TB        â”‚                               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                                                                      â”‚
â”‚  Pros: Simple, no code changes                                      â”‚
â”‚  Cons: Hardware limits, single point of failure, expensive          â”‚
â”‚                                                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”‚
â”‚                                                                      â”‚
â”‚  HORIZONTAL SCALING (Scale Out)                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                      â”‚
â”‚                                                                      â”‚
â”‚  Before:          After:                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ Server  â”‚      â”‚ Server  â”‚ â”‚ Server  â”‚ â”‚ Server  â”‚              â”‚
â”‚  â”‚    1    â”‚  â†’   â”‚    1    â”‚ â”‚    2    â”‚ â”‚    3    â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                                      â”‚
â”‚  Pros: No hardware limit, redundancy, cost-effective                â”‚
â”‚  Cons: Complexity, state management, data consistency               â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**When to use Vertical Scaling**:
- Early stage, simple systems
- Databases that are hard to shard
- When horizontal scaling adds too much complexity
- Quick fix while planning horizontal scaling

**When to use Horizontal Scaling**:
- High availability requirements
- Stateless services
- Beyond single-machine limits
- Cost optimization at scale

### Pattern 2: Database Scaling Patterns

#### Read Replicas

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    READ REPLICAS                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Application   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                             â”‚
           Writes                        Reads
              â”‚                             â”‚
              â–¼                             â–¼
```mermaid
flowchart LR
    Writes["Writes"] --> Primary["Primary DB"]
    Reads["Reads"] --> Replicas["Replicas<br/>(1, 2, 3)"]
    Primary -->|Replication| Replicas
```

<details>
<summary>ASCII diagram (reference)</summary>

```text
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Primary â”‚â”€â”€Replicationâ”€â–¶â”‚   Replicas    â”‚
        â”‚    DB    â”‚               â”‚   (1, 2, 3)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
</details>

  USE CASE: Read-heavy workloads (10:1 read/write ratio or higher)
  
  TRADE-OFF: Replication lag means reads might be slightly stale
```

#### Database Sharding

```mermaid
flowchart TD
    App["Application"] --> Router["Shard Router"]
    Router --> S1["Shard 1<br/>Users A-H"]
    Router --> S2["Shard 2<br/>Users I-P"]
    Router --> S3["Shard 3<br/>Users Q-Z"]
```

<details>
<summary>ASCII diagram (reference)</summary>

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATABASE SHARDING                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Application   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Shard Router   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                 â”‚                 â”‚
           â–¼                 â–¼                 â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Shard 1 â”‚       â”‚ Shard 2 â”‚       â”‚ Shard 3 â”‚
      â”‚Users A-Hâ”‚       â”‚Users I-Pâ”‚       â”‚Users Q-Zâ”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  SHARDING STRATEGIES:
  
  1. RANGE-BASED: Users A-H â†’ Shard 1
     Pros: Range queries efficient
     Cons: Hot spots (new users all in one shard)
  
  2. HASH-BASED: hash(user_id) % 3 â†’ Shard N
     Pros: Even distribution
     Cons: Range queries require all shards
  
  3. DIRECTORY-BASED: Lookup table maps key â†’ shard
     Pros: Flexible
     Cons: Lookup table is bottleneck
```
</details>

#### CQRS (Command Query Responsibility Segregation)

```mermaid
flowchart TD
    API["API Layer"] --> Commands["Commands<br/>(Writes)"]
    API --> Queries["Queries<br/>(Reads)"]
    Commands --> WriteDB["Write DB<br/>(PostgreSQL)<br/>Normalized"]
    Queries --> ReadDB["Read DB<br/>(Elasticsearch)<br/>Denormalized"]
    WriteDB -->|"Event Sync (Kafka)"| ReadDB
```

<details>
<summary>ASCII diagram (reference)</summary>

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CQRS FOR SCALING                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚    API Layer    â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                           â”‚
               Commands                      Queries
               (Writes)                      (Reads)
                    â”‚                           â”‚
                    â–¼                           â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚   Write DB     â”‚         â”‚    Read DB     â”‚
           â”‚  (PostgreSQL)  â”‚         â”‚(Elasticsearch) â”‚
           â”‚   Normalized   â”‚         â”‚ Denormalized   â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚                          â–²
                   â”‚                          â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        Event Sync (Kafka)

  USE CASE: Different read/write patterns, complex queries
  
  TRADE-OFF: Eventual consistency between read and write DBs
```
</details>

### Pattern 3: Caching Patterns for Scale

#### Multi-Tier Caching

```mermaid
flowchart TD
    Request["Request"] --> L1["L1: Browser/App Cache<br/>(Client-side cache)<br/>LocalStorage, memory"]
    L1 -->|Miss| L2["L2: CDN<br/>(Edge cache)<br/>CloudFront, Fastly"]
    L2 -->|Miss| L3["L3: Application Cache<br/>(Application cache)<br/>Local memory, Guava"]
    L3 -->|Miss| L4["L4: Distributed Cache<br/>(Distributed cache)<br/>Redis, Memcached"]
    L4 -->|Miss| L5["L5: Database<br/>(Source of truth)"]
```

<details>
<summary>ASCII diagram (reference)</summary>

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MULTI-TIER CACHING                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

         Request
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Browser/App  â”‚  â† L1: Client-side cache
    â”‚    Cache      â”‚     (LocalStorage, memory)
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚ Miss
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚      CDN      â”‚  â† L2: Edge cache
    â”‚               â”‚     (CloudFront, Fastly)
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚ Miss
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Application  â”‚  â† L3: Application cache
    â”‚    Cache      â”‚     (Local memory, Guava)
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚ Miss
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Distributed  â”‚  â† L4: Distributed cache
    â”‚    Cache      â”‚     (Redis, Memcached)
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚ Miss
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Database    â”‚  â† L5: Source of truth
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  CACHE HIT RATES (example):
  - L1 (Browser): 30%
  - L2 (CDN): 50% of remaining
  - L3 (App): 70% of remaining
  - L4 (Redis): 90% of remaining
  - L5 (DB): Only 1-2% of original requests
```
</details>

#### Cache-Aside vs Read-Through

**Cache-Aside Pattern:**

```mermaid
flowchart LR
    App1["Application"] --> Cache1["Cache"]
    App1 --> DB1["DB"]
```

Application manages cache explicitly:
1. Check cache
2. If miss, query DB
3. Store in cache
4. Return data

Pros: Application has full control  
Cons: Application must handle cache logic

**Read-Through Pattern:**

```mermaid
flowchart TD
    App2["Application"] --> Cache2["Cache"]
    Cache2 --> DB2["DB"]
```

Cache handles DB queries:
1. Application queries cache
2. Cache queries DB on miss
3. Cache stores and returns

Pros: Simpler application code  
Cons: Cache must understand data model

<details>
<summary>ASCII diagram (reference)</summary>

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CACHE-ASIDE PATTERN                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Application manages cache explicitly:
  
  1. Check cache
  2. If miss, query DB
  3. Store in cache
  4. Return data

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Application â”‚
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚         â”‚
    â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cache â”‚ â”‚  DB   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜

  Pros: Application has full control
  Cons: Application must handle cache logic


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    READ-THROUGH PATTERN                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Cache handles DB queries:
  
  1. Application queries cache
  2. Cache queries DB on miss
  3. Cache stores and returns

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Application â”‚
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Cache â”‚
    â””â”€â”€â”€â”¬â”€â”€â”€â”˜
        â”‚
        â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”
    â”‚  DB   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”˜

  Pros: Simpler application code
  Cons: Cache must understand data model
```
</details>

### Pattern 4: Message Queue Scaling

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MESSAGE QUEUE FOR SCALING                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  PROBLEM: Synchronous processing can't handle spikes

  SYNCHRONOUS:
  ```mermaid
  flowchart LR
      Client["Client"] --> Server["Server"]
      Server --> DB["DB"]
  ```
  
  If DB is slow, client waits. If spike occurs, system overloads.

  ASYNCHRONOUS WITH QUEUE:
  ```mermaid
  flowchart LR
      Client["Client"] --> Server["Server"]
      Server --> Queue["Queue"]
      Queue --> Workers["Workers"]
      Workers --> DB["DB"]
  ```
  
  Client gets immediate response. Queue absorbs spikes.

<details>
<summary>ASCII diagram (reference)</summary>

```text
  â”Œâ”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”
  â”‚Clientâ”‚â”€â”€â”€â–¶â”‚Serverâ”‚â”€â”€â”€â–¶â”‚ Queue â”‚â”€â”€â”€â–¶â”‚Workers â”‚â”€â”€â”€â–¶â”‚  DB  â”‚
  â””â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”˜
```
</details>
  Workers process at sustainable rate.

  SCALING WORKERS:
  
```mermaid
flowchart TD
    Queue["Queue<br/>(Kafka)"] --> W1["Worker 1"]
    Queue --> W2["Worker 2"]
    Queue --> W3["Worker 3"]
```

Add workers to increase throughput.  
Queue provides backpressure if workers can't keep up.

<details>
<summary>ASCII diagram (reference)</summary>

```text
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Queue   â”‚
                    â”‚  (Kafka)  â”‚
                    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                          â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                â”‚                â”‚
         â–¼                â–¼                â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚Worker 1 â”‚      â”‚Worker 2 â”‚      â”‚Worker 3 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
</details>

### Pattern 5: Microservices Decomposition

**Monolith:**

```mermaid
flowchart TD
    Monolith["Application<br/>User | Order | Payment | Notification<br/>(All in one service)"]
```

Scale everything together (even if only orders are busy)

**Microservices:**

```mermaid
flowchart LR
    subgraph Microservices["Independent Services"]
        UserSvc["User Service<br/>(2 inst)"]
        OrderSvc["Order Service<br/>(10 inst)"]
        PaymentSvc["Payment Service<br/>(3 inst)"]
        NotifSvc["Notif Service<br/>(5 inst)"]
    end
```

Scale each service independently based on load.

**WHEN TO DECOMPOSE:**
- Service has different scaling needs
- Service has different deployment frequency
- Service has different team ownership

<details>
<summary>ASCII diagram (reference)</summary>

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MICROSERVICES FOR SCALING                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  MONOLITH:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚           Application               â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”   â”‚
  â”‚  â”‚User â”‚ â”‚Orderâ”‚ â”‚Pay- â”‚ â”‚Notifâ”‚   â”‚
  â”‚  â”‚     â”‚ â”‚     â”‚ â”‚ment â”‚ â”‚     â”‚   â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
  Scale everything together (even if only orders are busy)

  MICROSERVICES:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  User   â”‚  â”‚  Order  â”‚  â”‚ Payment â”‚  â”‚  Notif  â”‚
  â”‚ Service â”‚  â”‚ Service â”‚  â”‚ Service â”‚  â”‚ Service â”‚
  â”‚ (2 inst)â”‚  â”‚(10 inst)â”‚  â”‚ (3 inst)â”‚  â”‚ (5 inst)â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
  Scale each service independently based on load.

  WHEN TO DECOMPOSE:
  - Service has different scaling needs
  - Service has different deployment frequency
  - Service has different team ownership
```
</details>
  - Service has different technology needs
  
  WHEN NOT TO DECOMPOSE:
  - Small team (< 10 engineers)
  - Simple domain
  - Low scale requirements
  - Tight coupling between components
```

### Pattern 6: CDN for Static Content

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CDN SCALING                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  WITHOUT CDN:
  
  User (Tokyo) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Server (US)
                     High latency (200ms)

  WITH CDN:
  
  User (Tokyo) â”€â”€â”€â–¶ CDN Edge (Tokyo) â”€â”€â”€â–¶ Server (US)
                    Low latency (20ms)   (Only on cache miss)

  CDN ARCHITECTURE:
  
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚  Origin  â”‚
           â”‚  Server  â”‚
           â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚           â”‚           â”‚
    â–¼           â–¼           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚ Edge  â”‚  â”‚ Edge  â”‚  â”‚ Edge  â”‚
â”‚  US   â”‚  â”‚  EU   â”‚  â”‚ Asia  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜
    â–²           â–²           â–²
    â”‚           â”‚           â”‚
 US Users    EU Users   Asia Users

  WHAT TO PUT ON CDN:
  - Static assets (JS, CSS, images)
  - Video content
  - API responses that don't change often
  - HTML pages (for static sites)
  
  WHAT NOT TO PUT ON CDN:
  - User-specific data
  - Real-time data
  - Sensitive information
```

---

## 4ï¸âƒ£ Simulation: Scaling Discussion in an Interview

**Interviewer**: "You've designed this e-commerce system for 100,000 users. How would you scale it to 10 million users?"

**Candidate**: "Let me identify the bottlenecks and address them systematically.

**Current Architecture**:
- Single PostgreSQL database
- 3 application servers behind a load balancer
- Redis cache
- Single region deployment

**Bottleneck Analysis at 10M users**:

1. **Database**: At 100x users, our database will be the first bottleneck. Currently handling 1,000 QPS, we'll need 100,000 QPS.

2. **Application Servers**: Can scale horizontally, but need to ensure they're stateless.

3. **Cache**: Redis can handle the load, but might need clustering for redundancy.

4. **Geographic Latency**: 10M users are likely global, single region won't work.

**Scaling Strategy**:

**Step 1: Database Scaling**

```
CURRENT:                    SCALED:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Postgres â”‚               â”‚ Primary  â”‚
â”‚ (Single) â”‚      â†’        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â–¼          â–¼          â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚Replica1â”‚ â”‚Replica2â”‚ â”‚Replica3â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- Add read replicas for read scaling (80% of queries are reads)
- This gets us to ~10x scale

For 100x, we need sharding:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Shard by user_id:                                                  â”‚
â”‚  - Users 0-3.3M â†’ Shard 1                                          â”‚
â”‚  - Users 3.3M-6.6M â†’ Shard 2                                       â”‚
â”‚  - Users 6.6M-10M â†’ Shard 3                                        â”‚
â”‚                                                                      â”‚
â”‚  Each shard has its own replicas for read scaling                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Step 2: Async Processing**

Move non-critical operations to async:

```
SYNCHRONOUS (before):
Order â†’ Payment â†’ Inventory â†’ Email â†’ Response

ASYNCHRONOUS (after):
Order â†’ Payment â†’ Inventory â†’ Response
                      â†“
                   [Queue]
                      â†“
                 Email Worker
```

This reduces response time and allows independent scaling of email sending.

**Step 3: Multi-Region Deployment**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                      â”‚
â”‚     US-EAST                              EU-WEST                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ App Servers   â”‚                   â”‚ App Servers   â”‚              â”‚
â”‚  â”‚ DB (Primary)  â”‚â—€â”€â”€Replicationâ”€â”€â”€â”€â–¶â”‚ DB (Replica)  â”‚              â”‚
â”‚  â”‚ Redis Cluster â”‚                   â”‚ Redis Cluster â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚         â–²                                   â–²                       â”‚
â”‚         â”‚                                   â”‚                       â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                     â”‚                                               â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚              â”‚ Global LB   â”‚                                        â”‚
â”‚              â”‚ (Route 53)  â”‚                                        â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Step 4: CDN for Static Content**

```
- Product images â†’ CloudFront
- JS/CSS bundles â†’ CloudFront
- Reduces origin load by 60-70%
```

**Summary of Changes**:

| Component | Before | After |
|-----------|--------|-------|
| Database | Single instance | Sharded + Replicas |
| App Servers | 3 instances | Auto-scaling (10-50) |
| Cache | Single Redis | Redis Cluster |
| Regions | 1 | 2+ |
| CDN | None | CloudFront |
| Processing | Synchronous | Async for non-critical |

**Trade-offs**:

1. **Sharding adds complexity**: Cross-shard queries are expensive. We'll need to denormalize some data.

2. **Multi-region adds latency for writes**: Writes go to primary region. We accept this for consistency.

3. **Async processing means eventual consistency**: User might not see email confirmation immediately. Acceptable for this use case.

Does this scaling approach make sense? Should I dive deeper into any component?"

---

## 5ï¸âƒ£ Quick Reference: Scaling Patterns

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SCALING PATTERNS QUICK REFERENCE                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  BOTTLENECK          PATTERN                  WHEN TO USE           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”‚
â”‚  CPU/Memory          Horizontal scaling       Stateless services    â”‚
â”‚  Database reads      Read replicas            Read-heavy workloads  â”‚
â”‚  Database writes     Sharding                 Write-heavy workloads â”‚
â”‚  Complex queries     CQRS                     Different R/W patternsâ”‚
â”‚  Latency             Caching (multi-tier)     Repeated data access  â”‚
â”‚  Geographic latency  CDN, Multi-region        Global users          â”‚
â”‚  Spike handling      Message queues           Unpredictable load    â”‚
â”‚  Service coupling    Microservices            Independent scaling   â”‚
â”‚                                                                      â”‚
â”‚  SCALING CHECKLIST                                                  â”‚
â”‚  â–¡ Identify the bottleneck first                                    â”‚
â”‚  â–¡ Start simple, add complexity as needed                           â”‚
â”‚  â–¡ Make services stateless for horizontal scaling                   â”‚
â”‚  â–¡ Use caching aggressively                                         â”‚
â”‚  â–¡ Move non-critical work to async                                  â”‚
â”‚  â–¡ Consider geographic distribution                                 â”‚
â”‚  â–¡ Monitor and measure to validate                                  â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 6ï¸âƒ£ Interview Follow-up Questions WITH Answers

### Q1: "How do you decide between vertical and horizontal scaling?"

**Answer**: "I start with vertical scaling for simplicity, then move to horizontal when we hit limits or need redundancy.

Vertical scaling is simpler (no code changes) but has limits (biggest machine is finite) and creates single points of failure.

Horizontal scaling requires the application to be stateless but provides redundancy and virtually unlimited scale.

For databases, vertical scaling is often the first step because horizontal scaling (sharding) adds significant complexity. For stateless application servers, horizontal scaling is usually straightforward and preferred."

### Q2: "What's the first thing you'd do to scale this system?"

**Answer**: "First, I'd measure to identify the actual bottleneck. Adding app servers doesn't help if the database is the bottleneck.

I'd look at:
- CPU/memory utilization of each component
- Database query times and QPS
- Network latency between components
- Queue depths if using async processing

Then I'd address the bottleneck with the simplest solution: caching for repeated reads, read replicas for read-heavy DB load, async processing for write spikes, or horizontal scaling for CPU-bound services."

### Q3: "How do you handle database sharding?"

**Answer**: "Sharding is a last resort because it adds significant complexity. Before sharding, I'd try:
1. Query optimization and indexing
2. Read replicas for read scaling
3. Caching to reduce DB load
4. Vertical scaling (bigger instance)

If sharding is necessary, I'd:
1. Choose a shard key that distributes evenly (usually user_id or tenant_id)
2. Use hash-based sharding for even distribution
3. Plan for cross-shard queries (they're expensive, so minimize them)
4. Consider using a sharding-aware database like CockroachDB or Vitess"

### Q4: "How do you scale writes?"

**Answer**: "Writes are harder to scale than reads because you can't just add replicas. Options include:

1. **Async writes**: Queue writes and process in background. Good for non-critical data.

2. **Batching**: Combine multiple writes into one. Good for high-volume, low-latency-tolerance scenarios.

3. **Sharding**: Distribute writes across multiple databases. Each shard handles a portion.

4. **Write-behind caching**: Write to cache, async flush to DB. Risky if cache fails.

5. **Event sourcing**: Write events to append-only log, derive state. Good for audit trails.

The choice depends on consistency requirements and acceptable latency."

---

## ğŸ”Ÿ One Clean Mental Summary

Scaling is about identifying and removing bottlenecks. Start simple, measure to find the actual bottleneck, then apply the appropriate pattern. The scaling ladder progresses from vertical scaling to horizontal scaling, read replicas, caching, sharding, async processing, and finally multi-region deployment.

Key patterns: horizontal scaling for stateless services, read replicas for read-heavy databases, sharding for write-heavy databases, multi-tier caching for repeated reads, message queues for spike handling, and CDN for static content.

Always consider the trade-offs: horizontal scaling adds complexity, sharding complicates queries, async processing means eventual consistency, and multi-region adds latency for writes. The goal is to scale just enough to meet requirements without over-engineering.

