# Change Data Capture (CDC): Streaming Database Changes

## 0ï¸âƒ£ Prerequisites

Before diving into Change Data Capture, you should understand:

- **Database Replication**: How databases replicate data using logs (covered in Topic 4).
- **Message Queues**: Kafka basics and pub/sub patterns (covered in Phase 6).
- **Event-Driven Architecture**: Events as the source of truth (covered in Phase 6).
- **Database Transactions**: Write-ahead logs and how databases persist changes (covered in Topic 7).

**Quick refresher on the problem**: Applications often need to react to database changesâ€”invalidate caches, update search indexes, sync data warehouses, or trigger workflows. Polling the database is inefficient. CDC captures changes as they happen and streams them to interested consumers.

---

## 1ï¸âƒ£ What Problem Does This Exist to Solve?

### The Specific Pain Point

Imagine you're running an e-commerce platform:

```
When a product price changes:
1. Update the database âœ“
2. Invalidate the cache (how?)
3. Update the search index (how?)
4. Notify the analytics system (how?)
5. Send price alert emails (how?)

Traditional approaches:

Approach 1: Application publishes events
  Problem: Dual-write problem
  - Write to DB succeeds
  - Publish to Kafka fails
  - Data is inconsistent!

Approach 2: Poll the database
  Problem: Inefficient and delayed
  - Query every second for changes
  - High database load
  - Still miss rapid changes

Approach 3: Database triggers
  Problem: Coupling and performance
  - Triggers slow down writes
  - Hard to maintain
  - Database becomes bottleneck
```

### The Dual-Write Problem

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 DUAL-WRITE PROBLEM                           â”‚
â”‚                                                              â”‚
â”‚  Application tries to:                                       â”‚
â”‚  1. Write to Database                                       â”‚
â”‚  2. Publish to Kafka                                        â”‚
â”‚                                                              â”‚
â”‚  Failure scenarios:                                          â”‚
â”‚                                                              â”‚
â”‚  Scenario A: DB succeeds, Kafka fails                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Database: Product price = $10                       â”‚   â”‚
â”‚  â”‚ Kafka: No event published                           â”‚   â”‚
â”‚  â”‚ Cache: Still shows old price                        â”‚   â”‚
â”‚  â”‚ Search: Still shows old price                       â”‚   â”‚
â”‚  â”‚ Result: INCONSISTENT                                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â”‚  Scenario B: Kafka succeeds, DB fails                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Database: Rollback, price unchanged                 â”‚   â”‚
â”‚  â”‚ Kafka: Event says price = $10                       â”‚   â”‚
â”‚  â”‚ Cache: Updated to $10 (wrong!)                      â”‚   â”‚
â”‚  â”‚ Result: INCONSISTENT                                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â”‚  CDC Solution: Database is source of truth                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ 1. Write to Database (single write)                 â”‚   â”‚
â”‚  â”‚ 2. CDC reads transaction log                        â”‚   â”‚
â”‚  â”‚ 3. CDC publishes to Kafka                           â”‚   â”‚
â”‚  â”‚ 4. Consumers update cache, search, etc.             â”‚   â”‚
â”‚  â”‚ Result: EVENTUALLY CONSISTENT (guaranteed)          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What CDC Enables

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CDC USE CASES                              â”‚
â”‚                                                              â”‚
â”‚  1. Cache Invalidation                                      â”‚
â”‚     Database changes â†’ CDC â†’ Invalidate Redis cache         â”‚
â”‚                                                              â”‚
â”‚  2. Search Index Updates                                    â”‚
â”‚     Database changes â†’ CDC â†’ Update Elasticsearch           â”‚
â”‚                                                              â”‚
â”‚  3. Data Warehouse Sync                                     â”‚
â”‚     OLTP database â†’ CDC â†’ Data warehouse (real-time ETL)    â”‚
â”‚                                                              â”‚
â”‚  4. Microservices Sync                                      â”‚
â”‚     Service A database â†’ CDC â†’ Service B reads events       â”‚
â”‚                                                              â”‚
â”‚  5. Audit Logging                                           â”‚
â”‚     All changes â†’ CDC â†’ Immutable audit log                 â”‚
â”‚                                                              â”‚
â”‚  6. Event Sourcing                                          â”‚
â”‚     Database as event store â†’ CDC â†’ Projections             â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Real Examples

**LinkedIn**: Uses CDC to keep search indexes in sync with the main database.

**Netflix**: CDC for real-time data replication across microservices.

**Uber**: CDC for syncing trip data to analytics systems.

**Airbnb**: CDC for cache invalidation and search updates.

---

## 2ï¸âƒ£ Intuition and Mental Model

### The Security Camera Analogy

**Without CDC = Periodic Inventory Checks**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PERIODIC INVENTORY CHECKS                       â”‚
â”‚                                                              â”‚
â”‚  Warehouse manager checks inventory every hour              â”‚
â”‚                                                              â”‚
â”‚  10:00 AM: Count = 100 units                               â”‚
â”‚  10:30 AM: Someone takes 20 units (unnoticed)              â”‚
â”‚  11:00 AM: Count = 80 units                                â”‚
â”‚                                                              â”‚
â”‚  Problem: Don't know WHEN the change happened              â”‚
â”‚           Don't know WHO made the change                    â”‚
â”‚           Might miss rapid changes between checks           â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**With CDC = Security Camera Recording**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SECURITY CAMERA RECORDING                       â”‚
â”‚                                                              â”‚
â”‚  Camera records every movement in real-time                 â”‚
â”‚                                                              â”‚
â”‚  10:00:00 - Inventory: 100 units                           â”‚
â”‚  10:15:23 - John removes 10 units                          â”‚
â”‚  10:22:45 - Jane adds 5 units                              â”‚
â”‚  10:30:12 - Mike removes 15 units                          â”‚
â”‚  10:45:00 - Inventory: 80 units                            â”‚
â”‚                                                              â”‚
â”‚  Benefits:                                                   â”‚
â”‚  - Know exactly WHEN each change happened                  â”‚
â”‚  - Know WHO made each change                               â”‚
â”‚  - Can replay history                                       â”‚
â”‚  - Real-time notifications possible                        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3ï¸âƒ£ How It Works Internally

### CDC Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CDC ARCHITECTURE                           â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚
â”‚  â”‚ Application â”‚                                            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                            â”‚
â”‚         â”‚ Write                                              â”‚
â”‚         â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚  Database   â”‚â”€â”€â”€â”€â–¶â”‚ Transaction â”‚                       â”‚
â”‚  â”‚  (MySQL,    â”‚     â”‚    Log      â”‚                       â”‚
â”‚  â”‚  PostgreSQL)â”‚     â”‚ (binlog,WAL)â”‚                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                             â”‚                                â”‚
â”‚                             â–¼                                â”‚
â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚                      â”‚  Debezium   â”‚ (CDC Connector)        â”‚
â”‚                      â”‚  Connector  â”‚                        â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                             â”‚                                â”‚
â”‚                             â–¼                                â”‚
â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚                      â”‚    Kafka    â”‚                        â”‚
â”‚                      â”‚   Topics    â”‚                        â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                             â”‚                                â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚         â–¼                   â–¼                   â–¼           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   Cache     â”‚    â”‚   Search    â”‚    â”‚    Data     â”‚     â”‚
â”‚  â”‚ Invalidator â”‚    â”‚   Indexer   â”‚    â”‚  Warehouse  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### How Databases Track Changes

**MySQL Binary Log (binlog)**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MYSQL BINLOG                              â”‚
â”‚                                                              â”‚
â”‚  Every write operation is logged:                           â”‚
â”‚                                                              â”‚
â”‚  binlog.000001:                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Position: 4                                          â”‚   â”‚
â”‚  â”‚ Event: QUERY                                         â”‚   â”‚
â”‚  â”‚ Statement: BEGIN                                     â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚ Position: 120                                        â”‚   â”‚
â”‚  â”‚ Event: TABLE_MAP                                     â”‚   â”‚
â”‚  â”‚ Table: products                                      â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚ Position: 180                                        â”‚   â”‚
â”‚  â”‚ Event: UPDATE_ROWS                                   â”‚   â”‚
â”‚  â”‚ Before: {id: 1, price: 100}                         â”‚   â”‚
â”‚  â”‚ After:  {id: 1, price: 90}                          â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚ Position: 250                                        â”‚   â”‚
â”‚  â”‚ Event: XID (commit)                                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â”‚  Formats:                                                    â”‚
â”‚  - STATEMENT: SQL statements (compact but can be ambiguous)â”‚
â”‚  - ROW: Actual row changes (larger but precise)            â”‚
â”‚  - MIXED: Combination                                       â”‚
â”‚                                                              â”‚
â”‚  CDC uses ROW format for accurate change capture           â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**PostgreSQL Write-Ahead Log (WAL)**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  POSTGRESQL WAL                              â”‚
â”‚                                                              â”‚
â”‚  Logical Decoding:                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ WAL records physical changes (page modifications)   â”‚   â”‚
â”‚  â”‚                                                      â”‚   â”‚
â”‚  â”‚ Logical decoding plugins translate to logical eventsâ”‚   â”‚
â”‚  â”‚ - pgoutput (built-in)                               â”‚   â”‚
â”‚  â”‚ - wal2json                                          â”‚   â”‚
â”‚  â”‚ - decoderbufs (Protobuf)                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â”‚  Replication Slot:                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Keeps track of consumer position                    â”‚   â”‚
â”‚  â”‚ Prevents WAL from being deleted before consumed     â”‚   â”‚
â”‚  â”‚ Named slot per consumer                             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â”‚  Example output (wal2json):                                 â”‚
â”‚  {                                                          â”‚
â”‚    "change": [                                              â”‚
â”‚      {                                                      â”‚
â”‚        "kind": "update",                                    â”‚
â”‚        "table": "products",                                 â”‚
â”‚        "columnnames": ["id", "price"],                     â”‚
â”‚        "columnvalues": [1, 90],                            â”‚
â”‚        "oldkeys": {"keynames": ["id"], "keyvalues": [1]}   â”‚
â”‚      }                                                      â”‚
â”‚    ]                                                        â”‚
â”‚  }                                                          â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Debezium Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  DEBEZIUM ARCHITECTURE                       â”‚
â”‚                                                              â”‚
â”‚  Kafka Connect Framework:                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                                                      â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚ Kafka Connect Worker                          â”‚  â”‚   â”‚
â”‚  â”‚  â”‚                                               â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ MySQL          â”‚  â”‚ PostgreSQL     â”‚     â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ Connector      â”‚  â”‚ Connector      â”‚     â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â”‚   â”‚
â”‚  â”‚  â”‚          â”‚                    â”‚              â”‚  â”‚   â”‚
â”‚  â”‚  â”‚          â–¼                    â–¼              â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â”‚ Converter (JSON, Avro, Protobuf)       â”‚ â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚   â”‚
â”‚  â”‚  â”‚                      â”‚                       â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚                         â”‚                           â”‚   â”‚
â”‚  â”‚                         â–¼                           â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚ Kafka Broker                                  â”‚  â”‚   â”‚
â”‚  â”‚  â”‚                                               â”‚  â”‚   â”‚
â”‚  â”‚  â”‚ Topics:                                       â”‚  â”‚   â”‚
â”‚  â”‚  â”‚ - dbserver1.inventory.products               â”‚  â”‚   â”‚
â”‚  â”‚  â”‚ - dbserver1.inventory.orders                 â”‚  â”‚   â”‚
â”‚  â”‚  â”‚ - dbserver1.inventory.customers              â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚                                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### CDC Event Structure

```json
{
  "schema": { ... },
  "payload": {
    "before": {
      "id": 1,
      "name": "Widget",
      "price": 100.00
    },
    "after": {
      "id": 1,
      "name": "Widget",
      "price": 90.00
    },
    "source": {
      "version": "2.0.0",
      "connector": "mysql",
      "name": "dbserver1",
      "ts_ms": 1678901234567,
      "db": "inventory",
      "table": "products",
      "server_id": 1,
      "file": "binlog.000003",
      "pos": 12345
    },
    "op": "u",  // c=create, u=update, d=delete, r=read (snapshot)
    "ts_ms": 1678901234600
  }
}
```

---

## 4ï¸âƒ£ Simulation-First Explanation

### Scenario 1: Cache Invalidation

```
Setup:
- MySQL database with products table
- Redis cache for product data
- Debezium capturing changes

T=0ms:   Application updates product price
         UPDATE products SET price = 90 WHERE id = 1;

T=1ms:   MySQL writes to binlog
         binlog: UPDATE products, id=1, price: 100â†’90

T=5ms:   Debezium reads binlog entry
         Creates change event

T=10ms:  Event published to Kafka
         Topic: dbserver1.inventory.products

T=15ms:  Cache invalidator consumes event
         Reads: op=update, table=products, id=1

T=20ms:  Cache invalidator deletes Redis key
         DEL product:1

T=25ms:  Next read request
         Cache miss â†’ Fetch from DB â†’ Cache new value

Total latency: ~25ms from DB write to cache invalidation
Guaranteed consistency (no dual-write problem)
```

### Scenario 2: Search Index Update

```
Setup:
- PostgreSQL with products table
- Elasticsearch for search
- Debezium + Kafka

T=0ms:   New product inserted
         INSERT INTO products (name, description, price) 
         VALUES ('Widget', 'A great widget', 99.99);

T=5ms:   PostgreSQL WAL entry created
         Logical decoding extracts change

T=10ms:  Debezium publishes to Kafka
         {
           "op": "c",
           "after": {
             "id": 123,
             "name": "Widget",
             "description": "A great widget",
             "price": 99.99
           }
         }

T=20ms:  Elasticsearch sink connector consumes
         Transforms to ES document

T=30ms:  Document indexed in Elasticsearch
         POST /products/_doc/123
         {
           "name": "Widget",
           "description": "A great widget",
           "price": 99.99
         }

T=35ms:  Product searchable!

User searches "widget" â†’ Finds new product
```

### Scenario 3: Initial Snapshot + Streaming

```
Setup:
- Existing database with 1M products
- New Debezium connector

Phase 1: Snapshot (initial load)
  T=0:      Debezium starts
  T=1s:     Locks table (briefly) to get consistent snapshot point
  T=2s:     Starts reading all rows
  T=5min:   1M rows read and published to Kafka
            Each event has op="r" (read/snapshot)

Phase 2: Streaming (ongoing changes)
  T=5min+:  Debezium switches to binlog streaming
  T=5min+1s: New insert captured
            Event has op="c" (create)
  
  Continues indefinitely...

Consumers see:
1. All existing data (snapshot)
2. All new changes (streaming)
3. No gaps, no duplicates (exactly-once with proper config)
```

---

## 5ï¸âƒ£ How Engineers Actually Use This in Production

### At Major Companies

**LinkedIn**:
- CDC for search index updates
- Databus (predecessor to Debezium)
- Billions of changes per day

**Uber**:
- CDC for real-time analytics
- Trip data synced to data warehouse
- Sub-minute latency

**Shopify**:
- CDC for inventory sync
- Multi-region data replication
- Cache invalidation

### Common Patterns

**Pattern 1: Outbox Pattern**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   OUTBOX PATTERN                             â”‚
â”‚                                                              â”‚
â”‚  Problem: Need to update DB and publish event atomically    â”‚
â”‚                                                              â”‚
â”‚  Solution:                                                   â”‚
â”‚  1. Write business data + outbox entry in same transaction â”‚
â”‚  2. CDC captures outbox table changes                       â”‚
â”‚  3. Publish outbox entries to Kafka                         â”‚
â”‚  4. Mark as published (or delete)                           â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Transaction:                                         â”‚   â”‚
â”‚  â”‚   INSERT INTO orders (...);                         â”‚   â”‚
â”‚  â”‚   INSERT INTO outbox (                              â”‚   â”‚
â”‚  â”‚     aggregate_type, aggregate_id, event_type, payloadâ”‚   â”‚
â”‚  â”‚   ) VALUES ('Order', 123, 'OrderCreated', '...');   â”‚   â”‚
â”‚  â”‚   COMMIT;                                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â”‚  CDC reads outbox â†’ Publishes to Kafka                     â”‚
â”‚  Guaranteed: Event published iff transaction committed     â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pattern 2: CQRS with CDC**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CQRS WITH CDC                              â”‚
â”‚                                                              â”‚
â”‚  Command Side:                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Application â†’ PostgreSQL (normalized, ACID)         â”‚   â”‚
â”‚  â”‚ Optimized for writes                                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â”‚                                   â”‚
â”‚                          â”‚ CDC                               â”‚
â”‚                          â–¼                                   â”‚
â”‚  Query Side:                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Kafka â†’ Elasticsearch (denormalized, fast reads)    â”‚   â”‚
â”‚  â”‚ Optimized for queries                               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â”‚  Writes go to PostgreSQL                                    â”‚
â”‚  Reads come from Elasticsearch                              â”‚
â”‚  CDC keeps them in sync                                     â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 6ï¸âƒ£ How to Implement or Apply It

### Debezium with Docker Compose

```yaml
# docker-compose.yml
version: '3.8'
services:
  mysql:
    image: mysql:8.0
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: inventory
    command: --server-id=1 --log-bin=mysql-bin --binlog-format=ROW
    ports:
      - "3306:3306"

  zookeeper:
    image: confluentinc/cp-zookeeper:7.0.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181

  kafka:
    image: confluentinc/cp-kafka:7.0.0
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    ports:
      - "9092:9092"

  connect:
    image: debezium/connect:2.0
    depends_on:
      - kafka
      - mysql
    environment:
      BOOTSTRAP_SERVERS: kafka:9092
      GROUP_ID: 1
      CONFIG_STORAGE_TOPIC: connect_configs
      OFFSET_STORAGE_TOPIC: connect_offsets
      STATUS_STORAGE_TOPIC: connect_statuses
    ports:
      - "8083:8083"
```

### Register Debezium Connector

```bash
# Register MySQL connector
curl -X POST http://localhost:8083/connectors \
  -H "Content-Type: application/json" \
  -d '{
    "name": "inventory-connector",
    "config": {
      "connector.class": "io.debezium.connector.mysql.MySqlConnector",
      "database.hostname": "mysql",
      "database.port": "3306",
      "database.user": "root",
      "database.password": "root",
      "database.server.id": "1",
      "topic.prefix": "dbserver1",
      "database.include.list": "inventory",
      "schema.history.internal.kafka.bootstrap.servers": "kafka:9092",
      "schema.history.internal.kafka.topic": "schema-changes.inventory"
    }
  }'
```

### Java Consumer for CDC Events

```java
package com.example.cdc;

import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.common.serialization.StringDeserializer;
import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;

import java.time.Duration;
import java.util.Collections;
import java.util.Properties;

public class CDCConsumer {
    
    private final ObjectMapper objectMapper = new ObjectMapper();
    private final CacheService cacheService;
    private final SearchService searchService;
    
    public void consumeChanges() {
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "cdc-consumer");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, 
            StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, 
            StringDeserializer.class.getName());
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        
        try (KafkaConsumer<String, String> consumer = 
                new KafkaConsumer<>(props)) {
            
            consumer.subscribe(Collections.singletonList(
                "dbserver1.inventory.products"));
            
            while (true) {
                ConsumerRecords<String, String> records = 
                    consumer.poll(Duration.ofMillis(100));
                
                for (ConsumerRecord<String, String> record : records) {
                    processChange(record.value());
                }
            }
        }
    }
    
    private void processChange(String eventJson) throws Exception {
        JsonNode event = objectMapper.readTree(eventJson);
        JsonNode payload = event.get("payload");
        
        String operation = payload.get("op").asText();
        String table = payload.get("source").get("table").asText();
        
        switch (operation) {
            case "c" -> handleCreate(table, payload.get("after"));
            case "u" -> handleUpdate(table, payload.get("before"), 
                                     payload.get("after"));
            case "d" -> handleDelete(table, payload.get("before"));
            case "r" -> handleSnapshot(table, payload.get("after"));
        }
    }
    
    private void handleCreate(String table, JsonNode after) {
        if ("products".equals(table)) {
            long productId = after.get("id").asLong();
            
            // Update cache
            cacheService.set("product:" + productId, after.toString());
            
            // Update search index
            searchService.index("products", productId, after);
        }
    }
    
    private void handleUpdate(String table, JsonNode before, JsonNode after) {
        if ("products".equals(table)) {
            long productId = after.get("id").asLong();
            
            // Invalidate cache (will be refreshed on next read)
            cacheService.delete("product:" + productId);
            
            // Update search index
            searchService.update("products", productId, after);
        }
    }
    
    private void handleDelete(String table, JsonNode before) {
        if ("products".equals(table)) {
            long productId = before.get("id").asLong();
            
            // Remove from cache
            cacheService.delete("product:" + productId);
            
            // Remove from search index
            searchService.delete("products", productId);
        }
    }
    
    private void handleSnapshot(String table, JsonNode after) {
        // Same as create, but during initial snapshot
        handleCreate(table, after);
    }
}
```

### Spring Boot with Debezium Embedded

```java
package com.example.cdc;

import io.debezium.config.Configuration;
import io.debezium.embedded.Connect;
import io.debezium.engine.DebeziumEngine;
import io.debezium.engine.RecordChangeEvent;
import io.debezium.engine.format.ChangeEventFormat;
import org.apache.kafka.connect.source.SourceRecord;
import org.springframework.stereotype.Component;

import jakarta.annotation.PostConstruct;
import jakarta.annotation.PreDestroy;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;

/**
 * Embedded Debezium engine (no Kafka Connect needed).
 */
@Component
public class EmbeddedDebeziumEngine {
    
    private DebeziumEngine<RecordChangeEvent<SourceRecord>> engine;
    private final ExecutorService executor = Executors.newSingleThreadExecutor();
    
    @PostConstruct
    public void start() {
        Configuration config = Configuration.create()
            .with("name", "embedded-engine")
            .with("connector.class", 
                "io.debezium.connector.mysql.MySqlConnector")
            .with("offset.storage", 
                "org.apache.kafka.connect.storage.FileOffsetBackingStore")
            .with("offset.storage.file.filename", "/tmp/offsets.dat")
            .with("offset.flush.interval.ms", 60000)
            .with("database.hostname", "localhost")
            .with("database.port", 3306)
            .with("database.user", "root")
            .with("database.password", "root")
            .with("database.server.id", 1)
            .with("topic.prefix", "embedded")
            .with("database.include.list", "inventory")
            .with("schema.history.internal", 
                "io.debezium.storage.file.history.FileSchemaHistory")
            .with("schema.history.internal.file.filename", 
                "/tmp/schema-history.dat")
            .build();
        
        engine = DebeziumEngine.create(ChangeEventFormat.of(Connect.class))
            .using(config.asProperties())
            .notifying(this::handleEvent)
            .build();
        
        executor.execute(engine);
    }
    
    private void handleEvent(RecordChangeEvent<SourceRecord> event) {
        SourceRecord record = event.record();
        
        if (record.value() != null) {
            // Process the change event
            System.out.println("Change detected: " + record.value());
            
            // Extract and process...
        }
    }
    
    @PreDestroy
    public void stop() throws Exception {
        if (engine != null) {
            engine.close();
        }
        executor.shutdown();
    }
}
```

### Outbox Pattern Implementation

```java
package com.example.cdc.outbox;

import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;
import com.fasterxml.jackson.databind.ObjectMapper;

@Entity
@Table(name = "outbox")
public class OutboxEvent {
    @Id
    @GeneratedValue(strategy = GenerationType.IDENTITY)
    private Long id;
    
    @Column(name = "aggregate_type")
    private String aggregateType;
    
    @Column(name = "aggregate_id")
    private String aggregateId;
    
    @Column(name = "event_type")
    private String eventType;
    
    @Column(name = "payload", columnDefinition = "JSON")
    private String payload;
    
    @Column(name = "created_at")
    private Instant createdAt = Instant.now();
}

@Service
public class OrderService {
    
    private final OrderRepository orderRepository;
    private final OutboxRepository outboxRepository;
    private final ObjectMapper objectMapper;
    
    @Transactional
    public Order createOrder(CreateOrderRequest request) {
        // Create order
        Order order = new Order(request);
        order = orderRepository.save(order);
        
        // Write to outbox (same transaction!)
        OutboxEvent event = new OutboxEvent();
        event.setAggregateType("Order");
        event.setAggregateId(order.getId().toString());
        event.setEventType("OrderCreated");
        event.setPayload(objectMapper.writeValueAsString(
            new OrderCreatedEvent(order)));
        outboxRepository.save(event);
        
        return order;
        // Debezium captures outbox insert
        // Publishes to Kafka
        // Guaranteed: Event published iff order created
    }
}

// Debezium connector config for outbox
// Uses outbox event router SMT (Single Message Transform)
```

---

## 7ï¸âƒ£ Tradeoffs, Pitfalls, and Common Mistakes

### Common Mistake 1: Not Handling Schema Changes

```
Scenario: Add column to table

Before:
  products (id, name, price)

After:
  products (id, name, price, description)

Problem:
- Old events don't have 'description'
- Consumers expecting it will break

Solution:
- Use schema registry (Avro, Protobuf)
- Handle missing fields gracefully
- Version your events
```

### Common Mistake 2: Ignoring Ordering

```
Scenario: Rapid updates to same row

T=0ms: UPDATE price = 100
T=1ms: UPDATE price = 90
T=2ms: UPDATE price = 80

If consumer processes out of order:
  Process: 80, 100, 90
  Final cache value: 90 (WRONG!)

Solution:
- Kafka preserves order within partition
- Use record key = table + primary key
- Same key â†’ same partition â†’ ordered
```

### Common Mistake 3: Replication Slot Bloat

```
Scenario: PostgreSQL CDC with slow consumer

Problem:
- Replication slot holds WAL
- Consumer is slow or stopped
- WAL grows indefinitely
- Disk fills up!

Solution:
- Monitor replication slot lag
- Alert on growing lag
- Set max_slot_wal_keep_size (PostgreSQL 13+)
- Have runbook for slot cleanup
```

### Common Mistake 4: Missing Initial Snapshot

```
Scenario: New consumer for existing data

Problem:
- Consumer starts reading from "now"
- Misses all existing data
- Incomplete view of the world

Solution:
- Debezium does initial snapshot by default
- Consumer handles both snapshot (op=r) and streaming
- Or: Backfill from database before starting CDC
```

### Performance Gotchas

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  CDC GOTCHAS                                 â”‚
â”‚                                                              â”‚
â”‚  1. Binlog/WAL overhead                                     â”‚
â”‚     - ROW format uses more disk                             â”‚
â”‚     - More data to replicate                                â”‚
â”‚     - Monitor disk usage                                    â”‚
â”‚                                                              â”‚
â”‚  2. Snapshot locking                                         â”‚
â”‚     - Initial snapshot may lock tables                      â”‚
â”‚     - Use snapshot.locking.mode = none (if safe)           â”‚
â”‚     - Schedule during low traffic                          â”‚
â”‚                                                              â”‚
â”‚  3. High cardinality topics                                  â”‚
â”‚     - One topic per table by default                        â”‚
â”‚     - Many tables = many topics                             â”‚
â”‚     - Consider topic routing                                â”‚
â”‚                                                              â”‚
â”‚  4. Large transactions                                       â”‚
â”‚     - Single transaction = single Kafka message             â”‚
â”‚     - Very large transactions may cause issues              â”‚
â”‚     - Break up large batch operations                       â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 8ï¸âƒ£ When NOT to Use This

### When NOT to Use CDC

1. **Simple applications**
   - Single database, no external systems
   - Application can handle all logic

2. **Real-time not needed**
   - Batch ETL is sufficient
   - Nightly sync is acceptable

3. **Source database doesn't support**
   - No binlog/WAL access
   - Managed database with restrictions

4. **Team lacks expertise**
   - Kafka operations are complex
   - Debugging CDC issues is hard

### Signs You Need CDC

- Multiple consumers need database changes
- Dual-write problems causing inconsistencies
- Polling is overloading the database
- Need real-time data synchronization
- Building event-driven architecture

---

## 9ï¸âƒ£ Comparison with Alternatives

### Data Sync Approaches

| Approach | Latency | Consistency | Complexity |
|----------|---------|-------------|------------|
| CDC | Seconds | Strong | High |
| Polling | Minutes | Weak | Low |
| Dual-write | Immediate | Weak | Medium |
| Triggers | Immediate | Strong | Medium |
| Batch ETL | Hours | Strong | Low |

### CDC Tools Comparison

| Tool | Databases | Deployment | License |
|------|-----------|------------|---------|
| Debezium | Many | Kafka Connect | Apache 2.0 |
| Maxwell | MySQL | Standalone | Apache 2.0 |
| DMS (AWS) | Many | Managed | Commercial |
| Striim | Many | Managed | Commercial |
| Fivetran | Many | Managed | Commercial |

---

## ğŸ”Ÿ Interview Follow-Up Questions WITH Answers

### L4 (Entry-Level) Questions

**Q1: What is CDC and why is it useful?**

**Answer:**
CDC (Change Data Capture) is a technique for capturing changes made to a database and streaming them to other systems in real-time.

**How it works:**
1. Database writes changes to a transaction log (binlog in MySQL, WAL in PostgreSQL)
2. CDC tool reads this log
3. Converts changes to events
4. Publishes to a message queue (like Kafka)
5. Consumers react to changes

**Why it's useful:**
- **Cache invalidation**: Automatically invalidate cache when data changes
- **Search sync**: Keep Elasticsearch in sync with database
- **Analytics**: Stream data to data warehouse in real-time
- **Microservices**: Share data between services without tight coupling

The key benefit is avoiding the dual-write problem: instead of writing to both database AND message queue (which can fail partially), you only write to the database and CDC handles the rest.

**Q2: What is the dual-write problem and how does CDC solve it?**

**Answer:**
The dual-write problem occurs when an application needs to update two systems atomically (e.g., database and message queue) but can't because they're separate systems.

**Example:**
```
1. Write to database â†’ Success
2. Publish to Kafka â†’ Fails (network error)
Result: Database updated, but no event published
        Other systems have stale data
```

**How CDC solves it:**
1. Application only writes to database (single write)
2. Database transaction either commits or rolls back (atomic)
3. CDC reads the transaction log
4. CDC publishes to Kafka
5. If CDC fails, it retries from the log (guaranteed delivery)

The database becomes the single source of truth. Events are guaranteed to be published for every committed transaction.

### L5 (Mid-Level) Questions

**Q3: How would you implement cache invalidation using CDC?**

**Answer:**
**Architecture:**
```
Database â†’ Debezium â†’ Kafka â†’ Cache Invalidator â†’ Redis
```

**Implementation steps:**

1. **Set up Debezium connector** for the database
2. **Create Kafka consumer** that subscribes to table topics
3. **Process change events:**
   - Extract table name and primary key
   - Delete corresponding cache key
   
4. **Handle edge cases:**
   - Batch deletes for efficiency
   - Handle consumer failures (Kafka handles retries)
   - Monitor lag between DB change and cache invalidation

**Code example:**
```java
void handleChange(ChangeEvent event) {
    if ("products".equals(event.getTable())) {
        String cacheKey = "product:" + event.getId();
        
        switch (event.getOperation()) {
            case CREATE, UPDATE -> redis.delete(cacheKey);
            case DELETE -> redis.delete(cacheKey);
        }
    }
}
```

**Why delete instead of update?**
- Simpler and safer
- Next read will fetch fresh data from DB
- Avoids race conditions with stale cache updates

**Q4: Explain the outbox pattern and when you would use it.**

**Answer:**
The outbox pattern ensures that database changes and event publishing happen atomically.

**How it works:**
1. In the same database transaction:
   - Write business data to main table
   - Write event to outbox table
2. CDC captures outbox table changes
3. CDC publishes events to Kafka
4. (Optional) Delete processed outbox entries

**Example:**
```sql
BEGIN;
INSERT INTO orders (id, user_id, total) VALUES (1, 123, 99.99);
INSERT INTO outbox (aggregate_type, aggregate_id, event_type, payload)
VALUES ('Order', '1', 'OrderCreated', '{"id":1,"userId":123}');
COMMIT;
```

**When to use:**
- Need guaranteed event publishing
- Building event-driven microservices
- Implementing saga pattern
- Can't afford inconsistency between DB and events

**Benefits:**
- Atomic: Event exists iff transaction committed
- Reliable: CDC handles retries
- Decoupled: Business logic doesn't know about Kafka

### L6 (Senior) Questions

**Q5: Design a CDC pipeline for a multi-region e-commerce platform.**

**Answer:**
**Requirements:**
- 3 regions: US, EU, Asia
- Product catalog updates need global sync
- Orders stay in region (data residency)
- Real-time inventory updates

**Architecture:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  MULTI-REGION CDC                            â”‚
â”‚                                                              â”‚
â”‚  US Region:                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Primary DB â†’ Debezium â†’ Kafka US                    â”‚   â”‚
â”‚  â”‚                           â”‚                          â”‚   â”‚
â”‚  â”‚                           â”œâ†’ Local consumers         â”‚   â”‚
â”‚  â”‚                           â””â†’ MirrorMaker â†’ Kafka EU  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â”‚  EU Region:                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Kafka EU â†’ Consumers â†’ Local cache/search           â”‚   â”‚
â”‚  â”‚         â†’ DB Sync (products only)                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â”‚  Topic routing:                                              â”‚
â”‚  - products.* â†’ Replicated globally                        â”‚
â”‚  - orders.* â†’ Stay in origin region                        â”‚
â”‚  - inventory.* â†’ Replicated for visibility                 â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key decisions:**

1. **Selective replication**: Only replicate what's needed globally
2. **Conflict resolution**: Products use last-write-wins (single source of truth in US)
3. **Latency**: Accept ~100ms lag for cross-region sync
4. **Ordering**: Use product ID as partition key for order preservation

**Monitoring:**
- Replication lag per region
- Consumer lag per topic
- Debezium connector health
- Kafka MirrorMaker lag

**Q6: How would you handle a CDC pipeline that's falling behind?**

**Answer:**
**Diagnosis:**

1. **Identify the bottleneck:**
   - Debezium connector lag
   - Kafka consumer lag
   - Downstream system (Elasticsearch, cache) slow

2. **Check metrics:**
   - Connector: `MilliSecondsBehindSource`
   - Consumer: `records-lag-max`
   - Kafka: `UnderReplicatedPartitions`

**Solutions by cause:**

**If Debezium is slow:**
- Increase `max.batch.size`
- Add more tasks (if supported)
- Check database performance
- Verify network bandwidth

**If Kafka consumers are slow:**
- Add more consumer instances
- Increase partition count
- Optimize consumer processing
- Use batch processing

**If downstream is slow:**
- Batch writes to Elasticsearch
- Use bulk Redis operations
- Add backpressure mechanism
- Consider async processing

**Emergency measures:**
- Skip non-critical events
- Pause non-essential consumers
- Scale infrastructure
- Alert on-call team

**Prevention:**
- Capacity planning based on write volume
- Load testing CDC pipeline
- Alerts on lag thresholds
- Runbooks for common issues

---

## 1ï¸âƒ£1ï¸âƒ£ One Clean Mental Summary

Change Data Capture (CDC) reads the database transaction log and streams changes to other systems. This solves the dual-write problem: instead of writing to both database and message queue (which can fail partially), you only write to the database and CDC handles event publishing.

Common uses: cache invalidation, search index updates, data warehouse sync, microservices communication.

Key tools: Debezium (open-source, runs on Kafka Connect), AWS DMS (managed), and database-specific tools like Maxwell (MySQL).

The outbox pattern combines CDC with a dedicated outbox table: write business data and event in the same transaction, CDC publishes the event. This guarantees events are published if and only if the transaction commits.

Watch out for: schema changes breaking consumers, ordering issues with rapid updates, replication slot bloat (PostgreSQL), and initial snapshot handling. Always monitor CDC lag and have runbooks for when it falls behind.

